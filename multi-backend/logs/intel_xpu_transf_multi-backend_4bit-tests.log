============================= test session starts ==============================
platform linux -- Python 3.10.14, pytest-8.2.2, pluggy-1.5.0 -- /home/sdp/.condax/mamba/envs/bnb/bin/python3.10
cachedir: .pytest_cache
rootdir: /home/sdp/src/transformers
configfile: pyproject.toml
collecting ... collected 43 items

tests/quantization/bnb/test_4bit.py::Bnb4BitTest::test_bnb_4bit_wrong_config FAILED [  2%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTest::test_device_and_dtype_assignment FAILED [  4%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTest::test_fp32_4bit_conversion FAILED [  6%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTest::test_generate_quality FAILED [  9%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTest::test_generate_quality_config FAILED [ 11%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTest::test_generate_quality_dequantize FAILED [ 13%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTest::test_linear_are_4bit FAILED [ 16%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTest::test_memory_footprint FAILED [ 18%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTest::test_original_dtype FAILED [ 20%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTest::test_quantization_config_json_serialization FAILED [ 23%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTest::test_quantization_num_parameters FAILED [ 25%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTest::test_rwkv_4bit FAILED  [ 27%]
tests/quantization/bnb/test_4bit.py::Bnb4BitT5Test::test_inference_with_keep_in_fp32 FAILED [ 30%]
tests/quantization/bnb/test_4bit.py::Bnb4BitT5Test::test_inference_without_keep_in_fp32 FAILED [ 32%]
tests/quantization/bnb/test_4bit.py::Classes4BitModelTest::test_correct_head_class FAILED [ 34%]
tests/quantization/bnb/test_4bit.py::Pipeline4BitTest::test_pipeline FAILED [ 37%]
tests/quantization/bnb/test_4bit.py::Pipeline4BitTest::test_pipeline ERROR [ 37%]
tests/quantization/bnb/test_4bit.py::Bnb4bitTestMultiGpu::test_multi_gpu_loading FAILED [ 39%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTestTraining::test_training FAILED [ 41%]
tests/quantization/bnb/test_4bit.py::Bnb4BitGPT2Test::test_bnb_4bit_wrong_config FAILED [ 44%]
tests/quantization/bnb/test_4bit.py::Bnb4BitGPT2Test::test_device_and_dtype_assignment FAILED [ 46%]
tests/quantization/bnb/test_4bit.py::Bnb4BitGPT2Test::test_fp32_4bit_conversion FAILED [ 48%]
tests/quantization/bnb/test_4bit.py::Bnb4BitGPT2Test::test_generate_quality FAILED [ 51%]
tests/quantization/bnb/test_4bit.py::Bnb4BitGPT2Test::test_generate_quality_config FAILED [ 53%]
tests/quantization/bnb/test_4bit.py::Bnb4BitGPT2Test::test_generate_quality_dequantize FAILED [ 55%]
tests/quantization/bnb/test_4bit.py::Bnb4BitGPT2Test::test_linear_are_4bit FAILED [ 58%]
tests/quantization/bnb/test_4bit.py::Bnb4BitGPT2Test::test_memory_footprint FAILED [ 60%]
tests/quantization/bnb/test_4bit.py::Bnb4BitGPT2Test::test_original_dtype FAILED [ 62%]
tests/quantization/bnb/test_4bit.py::Bnb4BitGPT2Test::test_quantization_config_json_serialization FAILED [ 65%]
tests/quantization/bnb/test_4bit.py::Bnb4BitGPT2Test::test_quantization_num_parameters FAILED [ 67%]
tests/quantization/bnb/test_4bit.py::Bnb4BitGPT2Test::test_rwkv_4bit FAILED [ 69%]
tests/quantization/bnb/test_4bit.py::BaseSerializationTest::test_serialization FAILED [ 72%]
tests/quantization/bnb/test_4bit.py::ExtendedSerializationTest::test_fp4_double_safe FAILED [ 74%]
tests/quantization/bnb/test_4bit.py::ExtendedSerializationTest::test_fp4_double_unsafe FAILED [ 76%]
tests/quantization/bnb/test_4bit.py::ExtendedSerializationTest::test_fp4_single_safe FAILED [ 79%]
tests/quantization/bnb/test_4bit.py::ExtendedSerializationTest::test_fp4_single_unsafe FAILED [ 81%]
tests/quantization/bnb/test_4bit.py::ExtendedSerializationTest::test_nf4_double_unsafe FAILED [ 83%]
tests/quantization/bnb/test_4bit.py::ExtendedSerializationTest::test_nf4_single_safe FAILED [ 86%]
tests/quantization/bnb/test_4bit.py::ExtendedSerializationTest::test_nf4_single_unsafe FAILED [ 88%]
tests/quantization/bnb/test_4bit.py::ExtendedSerializationTest::test_serialization FAILED [ 90%]
tests/quantization/bnb/test_4bit.py::BloomSerializationTest::test_serialization FAILED [ 93%]
tests/quantization/bnb/test_4bit.py::GPTSerializationTest::test_serialization FAILED [ 95%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTestBasicConfigTest::test_load_in_4_and_8_bit_fails PASSED [ 97%]
tests/quantization/bnb/test_4bit.py::Bnb4BitTestBasicConfigTest::test_set_load_in_8_bit PASSED [100%]

==================================== ERRORS ====================================
_____________ ERROR at teardown of Pipeline4BitTest.test_pipeline ______________

self = <bnb.test_4bit.Pipeline4BitTest testMethod=test_pipeline>

    def tearDown(self):
        r"""
        TearDown function needs to be called at the end of each test to free the GPU memory and cache, also to
        avoid unexpected behaviors. Please see: https://discuss.pytorch.org/t/how-can-we-release-gpu-memory-cache/14530/27
        """
>       del self.pipe
E       AttributeError: pipe

tests/quantization/bnb/test_4bit.py:453: AttributeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:06:01,585] [1/15] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
=================================== FAILURES ===================================
____________________ Bnb4BitTest.test_bnb_4bit_wrong_config ____________________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_bnb_4bit_wrong_config>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:38,473] [1/0] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
_________________ Bnb4BitTest.test_device_and_dtype_assignment _________________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_device_and_dtype_assignment>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:40,137] [1/1] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
____________________ Bnb4BitTest.test_fp32_4bit_conversion _____________________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_fp32_4bit_conversion>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:41,808] [1/2] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
______________________ Bnb4BitTest.test_generate_quality _______________________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_generate_quality>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:43,495] [1/3] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
___________________ Bnb4BitTest.test_generate_quality_config ___________________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_generate_quality_config>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:45,233] [1/4] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
_________________ Bnb4BitTest.test_generate_quality_dequantize _________________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_generate_quality_dequantize>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:47,047] [1/5] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
_______________________ Bnb4BitTest.test_linear_are_4bit _______________________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_linear_are_4bit>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:48,756] [1/6] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
______________________ Bnb4BitTest.test_memory_footprint _______________________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_memory_footprint>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:50,505] [1/7] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
_______________________ Bnb4BitTest.test_original_dtype ________________________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_original_dtype>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:52,151] [1/8] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
___________ Bnb4BitTest.test_quantization_config_json_serialization ____________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_quantization_config_json_serialization>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:53,789] [1/9] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
_________________ Bnb4BitTest.test_quantization_num_parameters _________________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_quantization_num_parameters>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:55,395] [1/10] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
__________________________ Bnb4BitTest.test_rwkv_4bit __________________________

self = <bnb.test_4bit.Bnb4BitTest testMethod=test_rwkv_4bit>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:57,144] [1/11] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
________________ Bnb4BitT5Test.test_inference_with_keep_in_fp32 ________________

self = <bnb.test_4bit.Bnb4BitT5Test testMethod=test_inference_with_keep_in_fp32>

    def test_inference_with_keep_in_fp32(self):
        r"""
        Test whether it is possible to mix both `4bit` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
        from transformers import T5ForConditionalGeneration
    
        # test with `google-t5/t5-small`
>       model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:377: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:58,209] [1/12] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
______________ Bnb4BitT5Test.test_inference_without_keep_in_fp32 _______________

self = <bnb.test_4bit.Bnb4BitT5Test testMethod=test_inference_without_keep_in_fp32>

    def test_inference_without_keep_in_fp32(self):
        r"""
        Test whether it is possible to mix both `4bit` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
        from transformers import T5ForConditionalGeneration
    
        modules = T5ForConditionalGeneration._keep_in_fp32_modules
        T5ForConditionalGeneration._keep_in_fp32_modules = None
    
        # test with `google-t5/t5-small`
>       model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:356: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:05:59,012] [1/13] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
_________________ Classes4BitModelTest.test_correct_head_class _________________

self = <bnb.test_4bit.Classes4BitModelTest testMethod=test_correct_head_class>

    def setUp(self):
        super().setUp()
        # model_name
        self.model_name = "bigscience/bloom-560m"
        self.seq_to_seq_name = "google-t5/t5-small"
    
        # Different types of model
    
>       self.base_model = AutoModel.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:403: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:06:00,303] [1/14] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
________________________ Pipeline4BitTest.test_pipeline ________________________

self = <bnb.test_4bit.Pipeline4BitTest testMethod=test_pipeline>

    def test_pipeline(self):
        r"""
        The aim of this test is to verify that the mixed 4bit is compatible with `pipeline` from transformers. Since
        we used pipline for inference speed benchmarking we want to make sure that this feature does not break anything
        on pipline.
        """
        # self._clear_cuda_cache()
>       self.pipe = pipeline(
            "text-generation",
            model=self.model_name,
            model_kwargs={
                "device_map": "auto",
                "load_in_4bit": True,
                # float16 isn't supported on CPU, use bfloat16 instead
                "torch_dtype": torch.bfloat16 if torch_device == "cpu" else torch.float16,
            },
            max_new_tokens=self.MAX_NEW_TOKENS,
        )

tests/quantization/bnb/test_4bit.py:465: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/pipelines/__init__.py:895: in pipeline
    framework, model = infer_framework_load_model(
src/transformers/pipelines/base.py:286: in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:06:01,585] [1/15] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
__________________ Bnb4bitTestMultiGpu.test_multi_gpu_loading __________________

self = <bnb.test_4bit.Bnb4bitTestMultiGpu testMethod=test_multi_gpu_loading>

    def test_multi_gpu_loading(self):
        r"""
        This tests that the model has been loaded and can be used correctly on a multi-GPU setup.
        Let's just try to load a model on 2 GPUs and see if it works. The model we test has ~2GB of total, 3GB should suffice
        """
    
>       model_parallel = AutoModelForCausalLM.from_pretrained(
            self.model_name, load_in_4bit=True, device_map="balanced"
        )

tests/quantization/bnb/test_4bit.py:494: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:237: in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:334: in to
    return self._quantize(device)
../bnb/bitsandbytes/nn/modules.py:296: in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
../bnb/bitsandbytes/functional.py:991: in quantize_4bit
    return backends[A.device.type].quantize_4bit(
../bnb/bitsandbytes/backends/cpu.py:142: in quantize_4bit
    return quantize_4bit_impl(A, absmax, out, blocksize, compress_statistics, quant_type)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328: in _fn
    return fn(*args, **kwargs)
../bnb/bitsandbytes/backends/cpu_xpu_common.py:317: in quantize_4bit_impl
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:490: in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:641: in _convert_frame
    result = inner_convert(frame, cache_size, hooks, frame_state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:133: in _fn
    return fn(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:389: in _convert_frame_assert
    return _compile(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:569: in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:189: in time_wrapper
    r = func(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:491: in compile_inner
    out_code = transform_code_object(code, transform)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028: in transform_code_object
    transformations(instructions, code_options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:458: in transform
    tracer.run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2074: in run
    super().run()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:724: in run
    and self.step()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:688: in step
    getattr(self, inst.opname)(inst)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:168: in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:600: in call_function
    res = binop_handler(tx, args[0], args[1], options)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:266: in dynamic_handler
    return wrap_fx_proxy(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1196: in wrap_fx_proxy
    return wrap_fx_proxy_cls(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1283: in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1376: in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1337: in get_fake_value
    return wrap_fake_exception(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:916: in wrap_fake_exception
    return fn()
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1338: in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1410: in run_node
    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_dynamo/utils.py:1397: in run_node
    return node.target(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1455: in binary_magic_impl
    return wrap_node(getattr(self.node, method_attr)(other_node))
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:825: in sub
    return self._sub(other)  # type: ignore[attr-defined]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1300: in binary_magic_impl
    out = func(self.expr, other.expr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = ((s0*s1)//s2), b = Mod(s0*s1, s2) > 0

>       'sub': lambda a, b: a - b,
        'mul': lambda a, b: a * b,
        'mod': lambda a, b: Mod(a, b),
        'pow': lambda a, b: Pow(a, b),
        'and': lambda a, b: sympy.And(a, b),
        'or': lambda a, b: sympy.Or(a, b),
        'truediv': lambda a, b: TrueDiv(a, b),
        'floordiv': lambda a, b: FloorDiv(a, b),
        'lshift': lambda a, b: LShift(a, b),
        'rshift': lambda a, b: RShift(a, b),
    }
E   torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function sub>(*(((s0*s1)//s2), Mod(s0*s1, s2) > 0), **{}):
E   unsupported operand type(s) for -: 'FloorDiv' and 'StrictGreaterThan'
E   
E   from user code:
E      File "/home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py", line 337, in <resume in quantize_4bit_impl>
E       absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
E   
E   Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
E   
E   
E   You can suppress this exception and fall back to eager by setting:
E       import torch._dynamo
E       torch._dynamo.config.suppress_errors = True

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:1040: TorchRuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-08-18 23:06:02,976] [1/16] torch.fx.experimental.symbolic_shapes: [WARNING] failed to eval sub(((s0*s1)//s2), Mod(s0*s1, s2) > 0)
______________________ Bnb4BitTestTraining.test_training _______________________

self = <bnb.test_4bit.Bnb4BitTestTraining testMethod=test_training>

    def test_training(self):
        if version.parse(importlib.metadata.version("bitsandbytes")) < version.parse("0.37.0"):
            self.skipTest(reason="This test requires bitsandbytes >= 0.37.0")
    
        # Step 1: freeze all parameters
        model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True)
    
        if torch.cuda.is_available():
            self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})
        else:
>           self.assertTrue(all(param.device.type == "cpu" for param in model.parameters()))
E           AssertionError: False is not true

tests/quantization/bnb/test_4bit.py:527: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
__________________ Bnb4BitGPT2Test.test_bnb_4bit_wrong_config __________________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_bnb_4bit_wrong_config>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:182: in create_quantized_param
    new_value = param_value.to(target_device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:06:05,843 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
_______________ Bnb4BitGPT2Test.test_device_and_dtype_assignment _______________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_device_and_dtype_assignment>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:182: in create_quantized_param
    new_value = param_value.to(target_device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:06:07,612 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
__________________ Bnb4BitGPT2Test.test_fp32_4bit_conversion ___________________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_fp32_4bit_conversion>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:182: in create_quantized_param
    new_value = param_value.to(target_device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:06:09,399 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
____________________ Bnb4BitGPT2Test.test_generate_quality _____________________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_generate_quality>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:182: in create_quantized_param
    new_value = param_value.to(target_device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:06:11,145 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
_________________ Bnb4BitGPT2Test.test_generate_quality_config _________________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_generate_quality_config>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:182: in create_quantized_param
    new_value = param_value.to(target_device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:06:12,749 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
_______________ Bnb4BitGPT2Test.test_generate_quality_dequantize _______________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_generate_quality_dequantize>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:182: in create_quantized_param
    new_value = param_value.to(target_device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:06:14,198 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
_____________________ Bnb4BitGPT2Test.test_linear_are_4bit _____________________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_linear_are_4bit>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:182: in create_quantized_param
    new_value = param_value.to(target_device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:06:15,732 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
____________________ Bnb4BitGPT2Test.test_memory_footprint _____________________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_memory_footprint>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:182: in create_quantized_param
    new_value = param_value.to(target_device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:06:17,254 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
_____________________ Bnb4BitGPT2Test.test_original_dtype ______________________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_original_dtype>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:182: in create_quantized_param
    new_value = param_value.to(target_device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:06:19,014 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
_________ Bnb4BitGPT2Test.test_quantization_config_json_serialization __________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_quantization_config_json_serialization>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:182: in create_quantized_param
    new_value = param_value.to(target_device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:06:20,720 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
_______________ Bnb4BitGPT2Test.test_quantization_num_parameters _______________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_quantization_num_parameters>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:182: in create_quantized_param
    new_value = param_value.to(target_device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:06:22,398 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
________________________ Bnb4BitGPT2Test.test_rwkv_4bit ________________________

self = <bnb.test_4bit.Bnb4BitGPT2Test testMethod=test_rwkv_4bit>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map="auto")

tests/quantization/bnb/test_4bit.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_4bit.py:182: in create_quantized_param
    new_value = param_value.to(target_device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:06:23,987 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
___________________ BaseSerializationTest.test_serialization ___________________

self = <bnb.test_4bit.BaseSerializationTest testMethod=test_serialization>
quant_type = 'nf4', double_quant = True, safe_serialization = True

    def test_serialization(self, quant_type="nf4", double_quant=True, safe_serialization=True):
        r"""
        Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.
        See ExtendedSerializationTest class for more params combinations.
        """
    
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=quant_type,
            bnb_4bit_use_double_quant=double_quant,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        model_0 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quantization_config,
            device_map=torch_device,
        )
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            model_0.save_pretrained(tmpdirname, safe_serialization=safe_serialization)
    
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
>           model_1 = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)

tests/quantization/bnb/test_4bit.py:604: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7fd3f94a7d90>
model = OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 768, padding_id...entwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=768, out_features=50272, bias=False)
)
param_value = tensor([[-0.0871,  0.0035, -0.0213,  ...,  0.0425, -0.0021,  0.0082],
        [ 0.0037,  0.0230,  0.0150,  ...,  0.020...,  0.0176, -0.0500],
        [-0.0155,  0.0334,  0.0111,  ...,  0.0387, -0.0299,  0.0230]],
       dtype=torch.float16)
param_name = 'model.decoder.layers.0.fc1.weight'
target_device = device(type='xpu')
state_dict = {'model.decoder.embed_positions.weight': tensor([[ 1.8272e-03,  9.0599e-04,  4.4289e-03,  ...,  1.6693e-02,
          ...889,
         1.0381,  1.0186,  1.0215,  1.0234,  1.0703,  1.0713,  1.0830,  1.0205],
       dtype=torch.float16), ...}
unexpected_keys = []

    def create_quantized_param(
        self,
        model: "PreTrainedModel",
        param_value: "torch.Tensor",
        param_name: str,
        target_device: "torch.device",
        state_dict: Dict[str, Any],
        unexpected_keys: Optional[List[str]] = None,
    ):
        """
        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()
        """
        import bitsandbytes as bnb
    
        module, tensor_name = get_module_from_name(model, param_name)
    
        if tensor_name not in module._parameters:
            raise ValueError(f"{module} does not have a parameter or a buffer named {tensor_name}.")
    
        old_value = getattr(module, tensor_name)
    
        if tensor_name == "bias":
            if param_value is None:
                new_value = old_value.to(target_device)
            else:
                new_value = param_value.to(target_device)
    
            new_value = torch.nn.Parameter(new_value, requires_grad=old_value.requires_grad)
            module._parameters[tensor_name] = new_value
            return
    
        if not isinstance(module._parameters[tensor_name], bnb.nn.Params4bit):
            raise ValueError("this function only loads `Linear4bit components`")
        if (
            old_value.device == torch.device("meta")
            and target_device not in ["meta", torch.device("meta")]
            and param_value is None
        ):
            raise ValueError(f"{tensor_name} is on the meta device, we need a `value` to put in on {target_device}.")
    
        # construct `new_value` for the module._parameters[tensor_name]:
        if self.pre_quantized:
            # 4bit loading. Collecting components for restoring quantized weight
            # This can be expanded to make a universal call for any quantized weight loading
    
            if not self.is_serializable:
                raise ValueError(
                    "Detected int4 weights but the version of bitsandbytes is not compatible with int4 serialization. "
                    "Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`."
                )
    
            if (param_name + ".quant_state.bitsandbytes__fp4" not in state_dict) and (
                param_name + ".quant_state.bitsandbytes__nf4" not in state_dict
            ):
>               raise ValueError(
                    f"Supplied state dict for {param_name} does not contain `bitsandbytes__*` and possibly other `quantized_stats` components."
                )
E               ValueError: Supplied state dict for model.decoder.layers.0.fc1.weight does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.

src/transformers/quantizers/quantizer_bnb_4bit.py:211: ValueError
----------------------------- Captured stderr call -----------------------------
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
________________ ExtendedSerializationTest.test_fp4_double_safe ________________

self = <bnb.test_4bit.ExtendedSerializationTest testMethod=test_fp4_double_safe>

    def test_fp4_double_safe(self):
>       self.test_serialization(quant_type="fp4", double_quant=True, safe_serialization=True)

tests/quantization/bnb/test_4bit.py:684: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
tests/quantization/bnb/test_4bit.py:604: in test_serialization
    model_1 = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7fd3f8ecf7c0>
model = OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 768, padding_id...entwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=768, out_features=50272, bias=False)
)
param_value = tensor([[-0.0871,  0.0035, -0.0213,  ...,  0.0425, -0.0021,  0.0082],
        [ 0.0037,  0.0230,  0.0150,  ...,  0.020...,  0.0176, -0.0500],
        [-0.0155,  0.0334,  0.0111,  ...,  0.0387, -0.0299,  0.0230]],
       dtype=torch.float16)
param_name = 'model.decoder.layers.0.fc1.weight'
target_device = device(type='xpu')
state_dict = {'model.decoder.embed_positions.weight': tensor([[ 1.8272e-03,  9.0599e-04,  4.4289e-03,  ...,  1.6693e-02,
          ...889,
         1.0381,  1.0186,  1.0215,  1.0234,  1.0703,  1.0713,  1.0830,  1.0205],
       dtype=torch.float16), ...}
unexpected_keys = []

    def create_quantized_param(
        self,
        model: "PreTrainedModel",
        param_value: "torch.Tensor",
        param_name: str,
        target_device: "torch.device",
        state_dict: Dict[str, Any],
        unexpected_keys: Optional[List[str]] = None,
    ):
        """
        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()
        """
        import bitsandbytes as bnb
    
        module, tensor_name = get_module_from_name(model, param_name)
    
        if tensor_name not in module._parameters:
            raise ValueError(f"{module} does not have a parameter or a buffer named {tensor_name}.")
    
        old_value = getattr(module, tensor_name)
    
        if tensor_name == "bias":
            if param_value is None:
                new_value = old_value.to(target_device)
            else:
                new_value = param_value.to(target_device)
    
            new_value = torch.nn.Parameter(new_value, requires_grad=old_value.requires_grad)
            module._parameters[tensor_name] = new_value
            return
    
        if not isinstance(module._parameters[tensor_name], bnb.nn.Params4bit):
            raise ValueError("this function only loads `Linear4bit components`")
        if (
            old_value.device == torch.device("meta")
            and target_device not in ["meta", torch.device("meta")]
            and param_value is None
        ):
            raise ValueError(f"{tensor_name} is on the meta device, we need a `value` to put in on {target_device}.")
    
        # construct `new_value` for the module._parameters[tensor_name]:
        if self.pre_quantized:
            # 4bit loading. Collecting components for restoring quantized weight
            # This can be expanded to make a universal call for any quantized weight loading
    
            if not self.is_serializable:
                raise ValueError(
                    "Detected int4 weights but the version of bitsandbytes is not compatible with int4 serialization. "
                    "Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`."
                )
    
            if (param_name + ".quant_state.bitsandbytes__fp4" not in state_dict) and (
                param_name + ".quant_state.bitsandbytes__nf4" not in state_dict
            ):
>               raise ValueError(
                    f"Supplied state dict for {param_name} does not contain `bitsandbytes__*` and possibly other `quantized_stats` components."
                )
E               ValueError: Supplied state dict for model.decoder.layers.0.fc1.weight does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.

src/transformers/quantizers/quantizer_bnb_4bit.py:211: ValueError
----------------------------- Captured stderr call -----------------------------
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
_______________ ExtendedSerializationTest.test_fp4_double_unsafe _______________

self = <bnb.test_4bit.ExtendedSerializationTest testMethod=test_fp4_double_unsafe>

    def test_fp4_double_unsafe(self):
>       self.test_serialization(quant_type="fp4", double_quant=True, safe_serialization=False)

tests/quantization/bnb/test_4bit.py:681: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
tests/quantization/bnb/test_4bit.py:604: in test_serialization
    model_1 = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7fd3f90217b0>
model = OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 768, padding_id...entwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=768, out_features=50272, bias=False)
)
param_value = tensor([[ 7.8796e-02,  9.4938e-04,  6.5308e-03,  ..., -4.6875e-02,
          3.9642e-02,  1.1843e-04],
        [ 2.036...   [-9.3460e-04, -2.8748e-02,  2.5269e-02,  ...,  3.5706e-02,
         -1.1932e-01,  3.1067e-02]], dtype=torch.float16)
param_name = 'model.decoder.layers.0.self_attn.k_proj.weight'
target_device = device(type='xpu')
state_dict = {'lm_head.weight': tensor([[ 0.1150, -0.1438,  0.0555,  ...,  0.2146,  0.0833,  0.0669],
        [ 0.1149, -0.1438,  0...e-02,  1.2598e-01, -1.2585e-01, -1.8530e-01,
        -2.5000e-01, -1.8347e-01,  1.7566e-01], dtype=torch.float16), ...}
unexpected_keys = []

    def create_quantized_param(
        self,
        model: "PreTrainedModel",
        param_value: "torch.Tensor",
        param_name: str,
        target_device: "torch.device",
        state_dict: Dict[str, Any],
        unexpected_keys: Optional[List[str]] = None,
    ):
        """
        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()
        """
        import bitsandbytes as bnb
    
        module, tensor_name = get_module_from_name(model, param_name)
    
        if tensor_name not in module._parameters:
            raise ValueError(f"{module} does not have a parameter or a buffer named {tensor_name}.")
    
        old_value = getattr(module, tensor_name)
    
        if tensor_name == "bias":
            if param_value is None:
                new_value = old_value.to(target_device)
            else:
                new_value = param_value.to(target_device)
    
            new_value = torch.nn.Parameter(new_value, requires_grad=old_value.requires_grad)
            module._parameters[tensor_name] = new_value
            return
    
        if not isinstance(module._parameters[tensor_name], bnb.nn.Params4bit):
            raise ValueError("this function only loads `Linear4bit components`")
        if (
            old_value.device == torch.device("meta")
            and target_device not in ["meta", torch.device("meta")]
            and param_value is None
        ):
            raise ValueError(f"{tensor_name} is on the meta device, we need a `value` to put in on {target_device}.")
    
        # construct `new_value` for the module._parameters[tensor_name]:
        if self.pre_quantized:
            # 4bit loading. Collecting components for restoring quantized weight
            # This can be expanded to make a universal call for any quantized weight loading
    
            if not self.is_serializable:
                raise ValueError(
                    "Detected int4 weights but the version of bitsandbytes is not compatible with int4 serialization. "
                    "Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`."
                )
    
            if (param_name + ".quant_state.bitsandbytes__fp4" not in state_dict) and (
                param_name + ".quant_state.bitsandbytes__nf4" not in state_dict
            ):
>               raise ValueError(
                    f"Supplied state dict for {param_name} does not contain `bitsandbytes__*` and possibly other `quantized_stats` components."
                )
E               ValueError: Supplied state dict for model.decoder.layers.0.self_attn.k_proj.weight does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.

src/transformers/quantizers/quantizer_bnb_4bit.py:211: ValueError
----------------------------- Captured stderr call -----------------------------
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
________________ ExtendedSerializationTest.test_fp4_single_safe ________________

self = <bnb.test_4bit.ExtendedSerializationTest testMethod=test_fp4_single_safe>

    def test_fp4_single_safe(self):
>       self.test_serialization(quant_type="fp4", double_quant=False, safe_serialization=True)

tests/quantization/bnb/test_4bit.py:678: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
tests/quantization/bnb/test_4bit.py:604: in test_serialization
    model_1 = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7fd3f8964640>
model = OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 768, padding_id...entwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=768, out_features=50272, bias=False)
)
param_value = tensor([[-0.0871,  0.0035, -0.0213,  ...,  0.0425, -0.0021,  0.0082],
        [ 0.0037,  0.0230,  0.0150,  ...,  0.020...,  0.0176, -0.0500],
        [-0.0155,  0.0334,  0.0111,  ...,  0.0387, -0.0299,  0.0230]],
       dtype=torch.float16)
param_name = 'model.decoder.layers.0.fc1.weight'
target_device = device(type='xpu')
state_dict = {'model.decoder.embed_positions.weight': tensor([[ 1.8272e-03,  9.0599e-04,  4.4289e-03,  ...,  1.6693e-02,
          ...889,
         1.0381,  1.0186,  1.0215,  1.0234,  1.0703,  1.0713,  1.0830,  1.0205],
       dtype=torch.float16), ...}
unexpected_keys = []

    def create_quantized_param(
        self,
        model: "PreTrainedModel",
        param_value: "torch.Tensor",
        param_name: str,
        target_device: "torch.device",
        state_dict: Dict[str, Any],
        unexpected_keys: Optional[List[str]] = None,
    ):
        """
        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()
        """
        import bitsandbytes as bnb
    
        module, tensor_name = get_module_from_name(model, param_name)
    
        if tensor_name not in module._parameters:
            raise ValueError(f"{module} does not have a parameter or a buffer named {tensor_name}.")
    
        old_value = getattr(module, tensor_name)
    
        if tensor_name == "bias":
            if param_value is None:
                new_value = old_value.to(target_device)
            else:
                new_value = param_value.to(target_device)
    
            new_value = torch.nn.Parameter(new_value, requires_grad=old_value.requires_grad)
            module._parameters[tensor_name] = new_value
            return
    
        if not isinstance(module._parameters[tensor_name], bnb.nn.Params4bit):
            raise ValueError("this function only loads `Linear4bit components`")
        if (
            old_value.device == torch.device("meta")
            and target_device not in ["meta", torch.device("meta")]
            and param_value is None
        ):
            raise ValueError(f"{tensor_name} is on the meta device, we need a `value` to put in on {target_device}.")
    
        # construct `new_value` for the module._parameters[tensor_name]:
        if self.pre_quantized:
            # 4bit loading. Collecting components for restoring quantized weight
            # This can be expanded to make a universal call for any quantized weight loading
    
            if not self.is_serializable:
                raise ValueError(
                    "Detected int4 weights but the version of bitsandbytes is not compatible with int4 serialization. "
                    "Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`."
                )
    
            if (param_name + ".quant_state.bitsandbytes__fp4" not in state_dict) and (
                param_name + ".quant_state.bitsandbytes__nf4" not in state_dict
            ):
>               raise ValueError(
                    f"Supplied state dict for {param_name} does not contain `bitsandbytes__*` and possibly other `quantized_stats` components."
                )
E               ValueError: Supplied state dict for model.decoder.layers.0.fc1.weight does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.

src/transformers/quantizers/quantizer_bnb_4bit.py:211: ValueError
----------------------------- Captured stderr call -----------------------------
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
_______________ ExtendedSerializationTest.test_fp4_single_unsafe _______________

self = <bnb.test_4bit.ExtendedSerializationTest testMethod=test_fp4_single_unsafe>

    def test_fp4_single_unsafe(self):
>       self.test_serialization(quant_type="fp4", double_quant=False, safe_serialization=False)

tests/quantization/bnb/test_4bit.py:675: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
tests/quantization/bnb/test_4bit.py:604: in test_serialization
    model_1 = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7fd424313eb0>
model = OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 768, padding_id...entwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=768, out_features=50272, bias=False)
)
param_value = tensor([[ 7.8796e-02,  9.4938e-04,  6.5308e-03,  ..., -4.6875e-02,
          3.9642e-02,  1.1843e-04],
        [ 2.036...   [-9.3460e-04, -2.8748e-02,  2.5269e-02,  ...,  3.5706e-02,
         -1.1932e-01,  3.1067e-02]], dtype=torch.float16)
param_name = 'model.decoder.layers.0.self_attn.k_proj.weight'
target_device = device(type='xpu')
state_dict = {'lm_head.weight': tensor([[ 0.1150, -0.1438,  0.0555,  ...,  0.2146,  0.0833,  0.0669],
        [ 0.1149, -0.1438,  0...e-02,  1.2598e-01, -1.2585e-01, -1.8530e-01,
        -2.5000e-01, -1.8347e-01,  1.7566e-01], dtype=torch.float16), ...}
unexpected_keys = []

    def create_quantized_param(
        self,
        model: "PreTrainedModel",
        param_value: "torch.Tensor",
        param_name: str,
        target_device: "torch.device",
        state_dict: Dict[str, Any],
        unexpected_keys: Optional[List[str]] = None,
    ):
        """
        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()
        """
        import bitsandbytes as bnb
    
        module, tensor_name = get_module_from_name(model, param_name)
    
        if tensor_name not in module._parameters:
            raise ValueError(f"{module} does not have a parameter or a buffer named {tensor_name}.")
    
        old_value = getattr(module, tensor_name)
    
        if tensor_name == "bias":
            if param_value is None:
                new_value = old_value.to(target_device)
            else:
                new_value = param_value.to(target_device)
    
            new_value = torch.nn.Parameter(new_value, requires_grad=old_value.requires_grad)
            module._parameters[tensor_name] = new_value
            return
    
        if not isinstance(module._parameters[tensor_name], bnb.nn.Params4bit):
            raise ValueError("this function only loads `Linear4bit components`")
        if (
            old_value.device == torch.device("meta")
            and target_device not in ["meta", torch.device("meta")]
            and param_value is None
        ):
            raise ValueError(f"{tensor_name} is on the meta device, we need a `value` to put in on {target_device}.")
    
        # construct `new_value` for the module._parameters[tensor_name]:
        if self.pre_quantized:
            # 4bit loading. Collecting components for restoring quantized weight
            # This can be expanded to make a universal call for any quantized weight loading
    
            if not self.is_serializable:
                raise ValueError(
                    "Detected int4 weights but the version of bitsandbytes is not compatible with int4 serialization. "
                    "Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`."
                )
    
            if (param_name + ".quant_state.bitsandbytes__fp4" not in state_dict) and (
                param_name + ".quant_state.bitsandbytes__nf4" not in state_dict
            ):
>               raise ValueError(
                    f"Supplied state dict for {param_name} does not contain `bitsandbytes__*` and possibly other `quantized_stats` components."
                )
E               ValueError: Supplied state dict for model.decoder.layers.0.self_attn.k_proj.weight does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.

src/transformers/quantizers/quantizer_bnb_4bit.py:211: ValueError
----------------------------- Captured stderr call -----------------------------
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
_______________ ExtendedSerializationTest.test_nf4_double_unsafe _______________

self = <bnb.test_4bit.ExtendedSerializationTest testMethod=test_nf4_double_unsafe>

    def test_nf4_double_unsafe(self):
>       self.test_serialization(quant_type="nf4", double_quant=True, safe_serialization=False)

tests/quantization/bnb/test_4bit.py:670: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
tests/quantization/bnb/test_4bit.py:604: in test_serialization
    model_1 = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7fd3f94f3f70>
model = OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 768, padding_id...entwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=768, out_features=50272, bias=False)
)
param_value = tensor([[ 7.8796e-02,  9.4938e-04,  6.5308e-03,  ..., -4.6875e-02,
          3.9642e-02,  1.1843e-04],
        [ 2.036...   [-9.3460e-04, -2.8748e-02,  2.5269e-02,  ...,  3.5706e-02,
         -1.1932e-01,  3.1067e-02]], dtype=torch.float16)
param_name = 'model.decoder.layers.0.self_attn.k_proj.weight'
target_device = device(type='xpu')
state_dict = {'lm_head.weight': tensor([[ 0.1150, -0.1438,  0.0555,  ...,  0.2146,  0.0833,  0.0669],
        [ 0.1149, -0.1438,  0...e-02,  1.2598e-01, -1.2585e-01, -1.8530e-01,
        -2.5000e-01, -1.8347e-01,  1.7566e-01], dtype=torch.float16), ...}
unexpected_keys = []

    def create_quantized_param(
        self,
        model: "PreTrainedModel",
        param_value: "torch.Tensor",
        param_name: str,
        target_device: "torch.device",
        state_dict: Dict[str, Any],
        unexpected_keys: Optional[List[str]] = None,
    ):
        """
        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()
        """
        import bitsandbytes as bnb
    
        module, tensor_name = get_module_from_name(model, param_name)
    
        if tensor_name not in module._parameters:
            raise ValueError(f"{module} does not have a parameter or a buffer named {tensor_name}.")
    
        old_value = getattr(module, tensor_name)
    
        if tensor_name == "bias":
            if param_value is None:
                new_value = old_value.to(target_device)
            else:
                new_value = param_value.to(target_device)
    
            new_value = torch.nn.Parameter(new_value, requires_grad=old_value.requires_grad)
            module._parameters[tensor_name] = new_value
            return
    
        if not isinstance(module._parameters[tensor_name], bnb.nn.Params4bit):
            raise ValueError("this function only loads `Linear4bit components`")
        if (
            old_value.device == torch.device("meta")
            and target_device not in ["meta", torch.device("meta")]
            and param_value is None
        ):
            raise ValueError(f"{tensor_name} is on the meta device, we need a `value` to put in on {target_device}.")
    
        # construct `new_value` for the module._parameters[tensor_name]:
        if self.pre_quantized:
            # 4bit loading. Collecting components for restoring quantized weight
            # This can be expanded to make a universal call for any quantized weight loading
    
            if not self.is_serializable:
                raise ValueError(
                    "Detected int4 weights but the version of bitsandbytes is not compatible with int4 serialization. "
                    "Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`."
                )
    
            if (param_name + ".quant_state.bitsandbytes__fp4" not in state_dict) and (
                param_name + ".quant_state.bitsandbytes__nf4" not in state_dict
            ):
>               raise ValueError(
                    f"Supplied state dict for {param_name} does not contain `bitsandbytes__*` and possibly other `quantized_stats` components."
                )
E               ValueError: Supplied state dict for model.decoder.layers.0.self_attn.k_proj.weight does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.

src/transformers/quantizers/quantizer_bnb_4bit.py:211: ValueError
----------------------------- Captured stderr call -----------------------------
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
________________ ExtendedSerializationTest.test_nf4_single_safe ________________

self = <bnb.test_4bit.ExtendedSerializationTest testMethod=test_nf4_single_safe>

    def test_nf4_single_safe(self):
>       self.test_serialization(quant_type="nf4", double_quant=False, safe_serialization=True)

tests/quantization/bnb/test_4bit.py:667: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
tests/quantization/bnb/test_4bit.py:604: in test_serialization
    model_1 = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7fd3f87a48b0>
model = OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 768, padding_id...entwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=768, out_features=50272, bias=False)
)
param_value = tensor([[-0.0871,  0.0035, -0.0213,  ...,  0.0425, -0.0021,  0.0082],
        [ 0.0037,  0.0230,  0.0150,  ...,  0.020...,  0.0176, -0.0500],
        [-0.0155,  0.0334,  0.0111,  ...,  0.0387, -0.0299,  0.0230]],
       dtype=torch.float16)
param_name = 'model.decoder.layers.0.fc1.weight'
target_device = device(type='xpu')
state_dict = {'model.decoder.embed_positions.weight': tensor([[ 1.8272e-03,  9.0599e-04,  4.4289e-03,  ...,  1.6693e-02,
          ...889,
         1.0381,  1.0186,  1.0215,  1.0234,  1.0703,  1.0713,  1.0830,  1.0205],
       dtype=torch.float16), ...}
unexpected_keys = []

    def create_quantized_param(
        self,
        model: "PreTrainedModel",
        param_value: "torch.Tensor",
        param_name: str,
        target_device: "torch.device",
        state_dict: Dict[str, Any],
        unexpected_keys: Optional[List[str]] = None,
    ):
        """
        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()
        """
        import bitsandbytes as bnb
    
        module, tensor_name = get_module_from_name(model, param_name)
    
        if tensor_name not in module._parameters:
            raise ValueError(f"{module} does not have a parameter or a buffer named {tensor_name}.")
    
        old_value = getattr(module, tensor_name)
    
        if tensor_name == "bias":
            if param_value is None:
                new_value = old_value.to(target_device)
            else:
                new_value = param_value.to(target_device)
    
            new_value = torch.nn.Parameter(new_value, requires_grad=old_value.requires_grad)
            module._parameters[tensor_name] = new_value
            return
    
        if not isinstance(module._parameters[tensor_name], bnb.nn.Params4bit):
            raise ValueError("this function only loads `Linear4bit components`")
        if (
            old_value.device == torch.device("meta")
            and target_device not in ["meta", torch.device("meta")]
            and param_value is None
        ):
            raise ValueError(f"{tensor_name} is on the meta device, we need a `value` to put in on {target_device}.")
    
        # construct `new_value` for the module._parameters[tensor_name]:
        if self.pre_quantized:
            # 4bit loading. Collecting components for restoring quantized weight
            # This can be expanded to make a universal call for any quantized weight loading
    
            if not self.is_serializable:
                raise ValueError(
                    "Detected int4 weights but the version of bitsandbytes is not compatible with int4 serialization. "
                    "Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`."
                )
    
            if (param_name + ".quant_state.bitsandbytes__fp4" not in state_dict) and (
                param_name + ".quant_state.bitsandbytes__nf4" not in state_dict
            ):
>               raise ValueError(
                    f"Supplied state dict for {param_name} does not contain `bitsandbytes__*` and possibly other `quantized_stats` components."
                )
E               ValueError: Supplied state dict for model.decoder.layers.0.fc1.weight does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.

src/transformers/quantizers/quantizer_bnb_4bit.py:211: ValueError
----------------------------- Captured stderr call -----------------------------
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
_______________ ExtendedSerializationTest.test_nf4_single_unsafe _______________

self = <bnb.test_4bit.ExtendedSerializationTest testMethod=test_nf4_single_unsafe>

    def test_nf4_single_unsafe(self):
>       self.test_serialization(quant_type="nf4", double_quant=False, safe_serialization=False)

tests/quantization/bnb/test_4bit.py:664: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
src/transformers/testing_utils.py:332: in wrapper
    return test_func(*args, **kwargs)
tests/quantization/bnb/test_4bit.py:604: in test_serialization
    model_1 = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7fd424415450>
model = OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 768, padding_id...entwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=768, out_features=50272, bias=False)
)
param_value = tensor([[ 7.8796e-02,  9.4938e-04,  6.5308e-03,  ..., -4.6875e-02,
          3.9642e-02,  1.1843e-04],
        [ 2.036...   [-9.3460e-04, -2.8748e-02,  2.5269e-02,  ...,  3.5706e-02,
         -1.1932e-01,  3.1067e-02]], dtype=torch.float16)
param_name = 'model.decoder.layers.0.self_attn.k_proj.weight'
target_device = device(type='xpu')
state_dict = {'lm_head.weight': tensor([[ 0.1150, -0.1438,  0.0555,  ...,  0.2146,  0.0833,  0.0669],
        [ 0.1149, -0.1438,  0...e-02,  1.2598e-01, -1.2585e-01, -1.8530e-01,
        -2.5000e-01, -1.8347e-01,  1.7566e-01], dtype=torch.float16), ...}
unexpected_keys = []

    def create_quantized_param(
        self,
        model: "PreTrainedModel",
        param_value: "torch.Tensor",
        param_name: str,
        target_device: "torch.device",
        state_dict: Dict[str, Any],
        unexpected_keys: Optional[List[str]] = None,
    ):
        """
        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()
        """
        import bitsandbytes as bnb
    
        module, tensor_name = get_module_from_name(model, param_name)
    
        if tensor_name not in module._parameters:
            raise ValueError(f"{module} does not have a parameter or a buffer named {tensor_name}.")
    
        old_value = getattr(module, tensor_name)
    
        if tensor_name == "bias":
            if param_value is None:
                new_value = old_value.to(target_device)
            else:
                new_value = param_value.to(target_device)
    
            new_value = torch.nn.Parameter(new_value, requires_grad=old_value.requires_grad)
            module._parameters[tensor_name] = new_value
            return
    
        if not isinstance(module._parameters[tensor_name], bnb.nn.Params4bit):
            raise ValueError("this function only loads `Linear4bit components`")
        if (
            old_value.device == torch.device("meta")
            and target_device not in ["meta", torch.device("meta")]
            and param_value is None
        ):
            raise ValueError(f"{tensor_name} is on the meta device, we need a `value` to put in on {target_device}.")
    
        # construct `new_value` for the module._parameters[tensor_name]:
        if self.pre_quantized:
            # 4bit loading. Collecting components for restoring quantized weight
            # This can be expanded to make a universal call for any quantized weight loading
    
            if not self.is_serializable:
                raise ValueError(
                    "Detected int4 weights but the version of bitsandbytes is not compatible with int4 serialization. "
                    "Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`."
                )
    
            if (param_name + ".quant_state.bitsandbytes__fp4" not in state_dict) and (
                param_name + ".quant_state.bitsandbytes__nf4" not in state_dict
            ):
>               raise ValueError(
                    f"Supplied state dict for {param_name} does not contain `bitsandbytes__*` and possibly other `quantized_stats` components."
                )
E               ValueError: Supplied state dict for model.decoder.layers.0.self_attn.k_proj.weight does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.

src/transformers/quantizers/quantizer_bnb_4bit.py:211: ValueError
----------------------------- Captured stderr call -----------------------------
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
_________________ ExtendedSerializationTest.test_serialization _________________

self = <bnb.test_4bit.ExtendedSerializationTest testMethod=test_serialization>
quant_type = 'nf4', double_quant = True, safe_serialization = True

    def test_serialization(self, quant_type="nf4", double_quant=True, safe_serialization=True):
        r"""
        Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.
        See ExtendedSerializationTest class for more params combinations.
        """
    
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=quant_type,
            bnb_4bit_use_double_quant=double_quant,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        model_0 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quantization_config,
            device_map=torch_device,
        )
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            model_0.save_pretrained(tmpdirname, safe_serialization=safe_serialization)
    
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
>           model_1 = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)

tests/quantization/bnb/test_4bit.py:604: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7fd3f8b76fe0>
model = OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 768, padding_id...entwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=768, out_features=50272, bias=False)
)
param_value = tensor([[-0.0871,  0.0035, -0.0213,  ...,  0.0425, -0.0021,  0.0082],
        [ 0.0037,  0.0230,  0.0150,  ...,  0.020...,  0.0176, -0.0500],
        [-0.0155,  0.0334,  0.0111,  ...,  0.0387, -0.0299,  0.0230]],
       dtype=torch.float16)
param_name = 'model.decoder.layers.0.fc1.weight'
target_device = device(type='xpu')
state_dict = {'model.decoder.embed_positions.weight': tensor([[ 1.8272e-03,  9.0599e-04,  4.4289e-03,  ...,  1.6693e-02,
          ...889,
         1.0381,  1.0186,  1.0215,  1.0234,  1.0703,  1.0713,  1.0830,  1.0205],
       dtype=torch.float16), ...}
unexpected_keys = []

    def create_quantized_param(
        self,
        model: "PreTrainedModel",
        param_value: "torch.Tensor",
        param_name: str,
        target_device: "torch.device",
        state_dict: Dict[str, Any],
        unexpected_keys: Optional[List[str]] = None,
    ):
        """
        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()
        """
        import bitsandbytes as bnb
    
        module, tensor_name = get_module_from_name(model, param_name)
    
        if tensor_name not in module._parameters:
            raise ValueError(f"{module} does not have a parameter or a buffer named {tensor_name}.")
    
        old_value = getattr(module, tensor_name)
    
        if tensor_name == "bias":
            if param_value is None:
                new_value = old_value.to(target_device)
            else:
                new_value = param_value.to(target_device)
    
            new_value = torch.nn.Parameter(new_value, requires_grad=old_value.requires_grad)
            module._parameters[tensor_name] = new_value
            return
    
        if not isinstance(module._parameters[tensor_name], bnb.nn.Params4bit):
            raise ValueError("this function only loads `Linear4bit components`")
        if (
            old_value.device == torch.device("meta")
            and target_device not in ["meta", torch.device("meta")]
            and param_value is None
        ):
            raise ValueError(f"{tensor_name} is on the meta device, we need a `value` to put in on {target_device}.")
    
        # construct `new_value` for the module._parameters[tensor_name]:
        if self.pre_quantized:
            # 4bit loading. Collecting components for restoring quantized weight
            # This can be expanded to make a universal call for any quantized weight loading
    
            if not self.is_serializable:
                raise ValueError(
                    "Detected int4 weights but the version of bitsandbytes is not compatible with int4 serialization. "
                    "Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`."
                )
    
            if (param_name + ".quant_state.bitsandbytes__fp4" not in state_dict) and (
                param_name + ".quant_state.bitsandbytes__nf4" not in state_dict
            ):
>               raise ValueError(
                    f"Supplied state dict for {param_name} does not contain `bitsandbytes__*` and possibly other `quantized_stats` components."
                )
E               ValueError: Supplied state dict for model.decoder.layers.0.fc1.weight does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.

src/transformers/quantizers/quantizer_bnb_4bit.py:211: ValueError
----------------------------- Captured stderr call -----------------------------
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
__________________ BloomSerializationTest.test_serialization ___________________

self = <bnb.test_4bit.BloomSerializationTest testMethod=test_serialization>
quant_type = 'nf4', double_quant = True, safe_serialization = True

    def test_serialization(self, quant_type="nf4", double_quant=True, safe_serialization=True):
        r"""
        Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.
        See ExtendedSerializationTest class for more params combinations.
        """
    
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=quant_type,
            bnb_4bit_use_double_quant=double_quant,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        model_0 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quantization_config,
            device_map=torch_device,
        )
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            model_0.save_pretrained(tmpdirname, safe_serialization=safe_serialization)
    
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
>           model_1 = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)

tests/quantization/bnb/test_4bit.py:604: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7fd3f8931930>
model = BloomForCausalLM(
  (transformer): BloomModel(
    (word_embeddings): Embedding(250880, 1024)
    (word_embeddings_lay...024,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)
)
param_value = tensor([[ 0.0050, -0.0342,  0.0131,  ...,  0.0070, -0.0012,  0.0206],
        [ 0.0031,  0.0061,  0.0025,  ...,  0.022..., -0.0032, -0.0374],
        [ 0.0010, -0.0225,  0.0035,  ...,  0.0048,  0.0047,  0.0234]],
       dtype=torch.float16)
param_name = 'transformer.h.0.mlp.dense_4h_to_h.weight'
target_device = device(type='xpu')
state_dict = {'transformer.h.0.input_layernorm.bias': tensor([-0.0662, -0.0798,  0.0445,  ..., -0.1449, -0.0452,  0.0142],
       d...032, -0.0374],
        [ 0.0010, -0.0225,  0.0035,  ...,  0.0048,  0.0047,  0.0234]],
       dtype=torch.float16), ...}
unexpected_keys = []

    def create_quantized_param(
        self,
        model: "PreTrainedModel",
        param_value: "torch.Tensor",
        param_name: str,
        target_device: "torch.device",
        state_dict: Dict[str, Any],
        unexpected_keys: Optional[List[str]] = None,
    ):
        """
        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()
        """
        import bitsandbytes as bnb
    
        module, tensor_name = get_module_from_name(model, param_name)
    
        if tensor_name not in module._parameters:
            raise ValueError(f"{module} does not have a parameter or a buffer named {tensor_name}.")
    
        old_value = getattr(module, tensor_name)
    
        if tensor_name == "bias":
            if param_value is None:
                new_value = old_value.to(target_device)
            else:
                new_value = param_value.to(target_device)
    
            new_value = torch.nn.Parameter(new_value, requires_grad=old_value.requires_grad)
            module._parameters[tensor_name] = new_value
            return
    
        if not isinstance(module._parameters[tensor_name], bnb.nn.Params4bit):
            raise ValueError("this function only loads `Linear4bit components`")
        if (
            old_value.device == torch.device("meta")
            and target_device not in ["meta", torch.device("meta")]
            and param_value is None
        ):
            raise ValueError(f"{tensor_name} is on the meta device, we need a `value` to put in on {target_device}.")
    
        # construct `new_value` for the module._parameters[tensor_name]:
        if self.pre_quantized:
            # 4bit loading. Collecting components for restoring quantized weight
            # This can be expanded to make a universal call for any quantized weight loading
    
            if not self.is_serializable:
                raise ValueError(
                    "Detected int4 weights but the version of bitsandbytes is not compatible with int4 serialization. "
                    "Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`."
                )
    
            if (param_name + ".quant_state.bitsandbytes__fp4" not in state_dict) and (
                param_name + ".quant_state.bitsandbytes__nf4" not in state_dict
            ):
>               raise ValueError(
                    f"Supplied state dict for {param_name} does not contain `bitsandbytes__*` and possibly other `quantized_stats` components."
                )
E               ValueError: Supplied state dict for transformer.h.0.mlp.dense_4h_to_h.weight does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.

src/transformers/quantizers/quantizer_bnb_4bit.py:211: ValueError
----------------------------- Captured stderr call -----------------------------
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
___________________ GPTSerializationTest.test_serialization ____________________

self = <bnb.test_4bit.GPTSerializationTest testMethod=test_serialization>
quant_type = 'nf4', double_quant = True, safe_serialization = True

    def test_serialization(self, quant_type="nf4", double_quant=True, safe_serialization=True):
        r"""
        Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.
        See ExtendedSerializationTest class for more params combinations.
        """
    
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=quant_type,
            bnb_4bit_use_double_quant=double_quant,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        model_0 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.quantization_config,
            device_map=torch_device,
        )
    
        with tempfile.TemporaryDirectory() as tmpdirname:
>           model_0.save_pretrained(tmpdirname, safe_serialization=safe_serialization)

tests/quantization/bnb/test_4bit.py:599: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/modeling_utils.py:2772: in save_pretrained
    safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={"format": "pt"})
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/safetensors/torch.py:286: in save_file
    serialize_file(_flatten(tensors), filename, metadata=metadata)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/safetensors/torch.py:496: in _flatten
    return {
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/safetensors/torch.py:500: in <dictcomp>
    "data": _tobytes(v, k),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = tensor([[-0.0413, -0.0078,  0.1652,  ..., -0.0733,  0.0389, -0.0545],
        [-0.0058, -0.0298, -0.0466,  ..., -0.056...20],
        [ 0.0150,  0.0370,  0.0300,  ...,  0.0014,  0.0117,  0.0563]],
       device='xpu:0', dtype=torch.float16)
name = 'transformer.h.0.attn.c_attn.weight'

    def _tobytes(tensor: torch.Tensor, name: str) -> bytes:
        if tensor.layout != torch.strided:
            raise ValueError(
                f"You are trying to save a sparse tensor: `{name}` which this library does not support."
                " You can make it a dense tensor before saving with `.to_dense()` but be aware this might"
                " make a much larger file than needed."
            )
    
        if not tensor.is_contiguous():
>           raise ValueError(
                f"You are trying to save a non contiguous tensor: `{name}` which is not allowed. It either means you"
                " are trying to save tensors which are reference of each other in which case it's recommended to save"
                " only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to"
                " pack it before saving."
            )
E           ValueError: You are trying to save a non contiguous tensor: `transformer.h.0.attn.c_attn.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving.

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/safetensors/torch.py:414: ValueError
=============================== warnings summary ===============================
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/utils/cpp_extension.py:28
  /home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/utils/cpp_extension.py:28: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    from pkg_resources import packaging  # type: ignore[attr-defined]

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/_pytest/config/__init__.py:1448
  /home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/_pytest/config/__init__.py:1448: PytestConfigWarning: Unknown config option: doctest_glob
  
    self._warn_or_fail_if_strict(f"Unknown config option: {key}\n")

tests/quantization/bnb/test_4bit.py: 17 warnings
  /home/sdp/src/bnb/bitsandbytes/backends/cpu_xpu_common.py:317: UserWarning: fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.
    warnings.warn("fp4 quantization is currently slow on CPU/XPU. Please Use nf4 instead for better performance.")

tests/quantization/bnb/test_4bit.py: 23 warnings
  /home/sdp/src/transformers/src/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
    warnings.warn(

tests/quantization/bnb/test_4bit.py::Bnb4BitTestTraining::test_training
  /home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
    return self.fget.__get__(instance, owner)()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
======== 41 failed, 2 passed, 43 warnings, 1 error in 66.41s (0:01:06) =========
