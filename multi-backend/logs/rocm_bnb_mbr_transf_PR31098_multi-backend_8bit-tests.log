============================= test session starts ==============================
platform linux -- Python 3.8.19, pytest-8.3.2, pluggy-1.5.0 -- /home/huggingface/.condax/mamba/envs/bnb/bin/python3.8
cachedir: .pytest_cache
rootdir: /home/huggingface/src/transformers
configfile: pyproject.toml
collecting ... collected 45 items

tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_device_and_dtype_assignment PASSED [  2%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_fp32_int8_conversion PASSED [  4%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_generate_quality FAILED [  6%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_generate_quality_config FAILED [  8%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_generate_quality_dequantize FAILED [ 11%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert PASSED [ 13%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert_trust_remote_code PASSED [ 15%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_int8_from_pretrained FAILED [ 17%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_int8_serialization FAILED [ 20%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_int8_serialization_regression FAILED [ 22%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_int8_serialization_sharded FAILED [ 24%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_linear_are_8bit PASSED [ 26%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_llm_skip PASSED [ 28%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_memory_footprint PASSED [ 31%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_original_dtype PASSED [ 33%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_quantization_config_json_serialization PASSED [ 35%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_raise_if_config_and_load_in_8bit PASSED [ 37%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8T5Test::test_inference_with_keep_in_fp32 FAILED [ 40%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8T5Test::test_inference_with_keep_in_fp32_serialized FAILED [ 42%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8T5Test::test_inference_without_keep_in_fp32 FAILED [ 44%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8ModelClassesTest::test_correct_head_class PASSED [ 46%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestPipeline::test_pipeline FAILED [ 48%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestMultiGpu::test_multi_gpu_loading FAILED [ 51%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_disk_loading_custom_device_map FAILED [ 53%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_disk_loading_custom_device_map_kwargs FAILED [ 55%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_loading_custom_device_map FAILED [ 57%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_loading_random_device_map FAILED [ 60%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestTraining::test_training FAILED [ 62%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_device_and_dtype_assignment PASSED [ 64%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_fp32_int8_conversion FAILED [ 66%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_generate_quality FAILED [ 68%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_generate_quality_config FAILED [ 71%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_generate_quality_dequantize FAILED [ 73%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_get_keys_to_not_convert PASSED [ 75%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_get_keys_to_not_convert_trust_remote_code PASSED [ 77%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_from_pretrained FAILED [ 80%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization FAILED [ 82%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization_regression FAILED [ 84%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization_sharded FAILED [ 86%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_linear_are_8bit PASSED [ 88%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_llm_skip PASSED [ 91%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_memory_footprint PASSED [ 93%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_original_dtype PASSED [ 95%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_quantization_config_json_serialization PASSED [ 97%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_raise_if_config_and_load_in_8bit PASSED [100%]

=================================== FAILURES ===================================
_____________________ MixedInt8Test.test_generate_quality ______________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_generate_quality>

    def test_generate_quality(self):
        r"""
        Test the generation quality of the quantized model and see that we are matching the expected output.
        Given that we are operating on small numbers + the testing model is relatively small, we might not get
        the same output across GPUs. So we'll generate few tokens (5-10) and check their output.
        """
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = self.model_8bit.generate(
            input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
        )

tests/quantization/bnb/test_mixed_int8.py:273: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[-41,  25,   2,  ...,  14, -26, -29],
        [-52,  20,  10,  ..., -21,  -5,  15],
        [ 34,   9,  -3,  .... 0,   0,  ...,   0,   0,   0],
        [  0,   0,   0,  ...,   0,   0,   0]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
__________________ MixedInt8Test.test_generate_quality_config __________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_generate_quality_config>

    def test_generate_quality_config(self):
        r"""
        Test that loading the model with the config is equivalent
        """
        bnb_config = BitsAndBytesConfig()
        bnb_config.load_in_8bit = True
    
        model_8bit_from_config = AutoModelForCausalLM.from_pretrained(
            self.model_name, quantization_config=bnb_config, device_map="auto"
        )
    
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model_8bit_from_config.generate(
            input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
        )

tests/quantization/bnb/test_mixed_int8.py:291: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[-41,  25,   2,  ...,  14, -26, -29],
        [-52,  20,  10,  ..., -21,  -5,  15],
        [ 34,   9,  -3,  ....-8,  24,  ..., -25,  19,  46],
        [ 24,  44,  61,  ..., -23, -42, -34]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
________________ MixedInt8Test.test_generate_quality_dequantize ________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_generate_quality_dequantize>

    def test_generate_quality_dequantize(self):
        r"""
        Test that loading the model and dequantizing it produce correct results
        """
        bnb_config = BitsAndBytesConfig(load_in_8bit=True)
    
        model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name, quantization_config=bnb_config, device_map="auto"
        )
    
>       model_8bit.dequantize()

tests/quantization/bnb/test_mixed_int8.py:307: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/modeling_utils.py:1420: in dequantize
    return hf_quantizer.dequantize(self)
src/transformers/quantizers/base.py:202: in dequantize
    model = self._dequantize(model)
src/transformers/quantizers/quantizer_bnb_8bit.py:313: in _dequantize
    model = dequantize_and_replace(
src/transformers/integrations/bitsandbytes.py:460: in dequantize_and_replace
    model, has_been_replaced = _dequantize_and_replace(
src/transformers/integrations/bitsandbytes.py:442: in _dequantize_and_replace
    _, has_been_replaced = _dequantize_and_replace(
src/transformers/integrations/bitsandbytes.py:442: in _dequantize_and_replace
    _, has_been_replaced = _dequantize_and_replace(
src/transformers/integrations/bitsandbytes.py:442: in _dequantize_and_replace
    _, has_been_replaced = _dequantize_and_replace(
src/transformers/integrations/bitsandbytes.py:442: in _dequantize_and_replace
    _, has_been_replaced = _dequantize_and_replace(
src/transformers/integrations/bitsandbytes.py:426: in _dequantize_and_replace
    new_module.weight = torch.nn.Parameter(dequantize_bnb_weight(module.weight, dtype, state))
src/transformers/integrations/bitsandbytes.py:363: in dequantize_bnb_weight
    out32, Sout32 = bnb.functional.igemmlt(im, state.CxB, Sim, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[127,   0,   0,  ...,   0,   0,   0],
        [  0, 127,   0,  ...,   0,   0,   0],
        [  0,   0, 127,  .... 0,   0,  ...,   0, 127,   0],
        [  0,   0,   0,  ...,   0,   0, 127]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[-41,  25,   2,  ...,  14, -26, -29],
        [-52,  20,  10,  ..., -21,  -5,  15],
        [ 34,   9,  -3,  ....-8,  24,  ..., -25,  19,  46],
        [ 24,  44,  61,  ..., -23, -42, -34]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([2048, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
   ......, 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((2048, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([2048, 2048]), B: torch.Size([6144, 2048]), C: (2048, 6144); (lda, ldb, ldc): (c_int(2048), c_int(6144), c_int(2048)); (m, n, k): (c_int(2048), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
___________________ MixedInt8Test.test_int8_from_pretrained ____________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_int8_from_pretrained>

    def test_int8_from_pretrained(self):
        r"""
        Test whether loading a 8bit model from the Hub works as expected
        """
        from bitsandbytes.nn import Int8Params
    
        model_id = "ybelkada/bloom-1b7-8bit"
    
        model = AutoModelForCausalLM.from_pretrained(model_id)
    
        linear = get_some_linear_layer(model)
        self.assertTrue(linear.weight.__class__ == Int8Params)
        self.assertTrue(hasattr(linear.weight, "SCB"))
    
        # generate
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model.generate(input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10)

tests/quantization/bnb/test_mixed_int8.py:477: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[  0, -42,   8,  ...,  -4,  18,  -7],
        [ -3, -29,  22,  ...,  -9,  45,  10],
        [ -2,  16, -44,  ...,  -9, -51, -25],
        [  4,  11,  19,  ...,  -4,   5, -15]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ....16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Unused kwargs: ['_from_model_config', 'transformers_version']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
____________________ MixedInt8Test.test_int8_serialization _____________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_int8_serialization>

    def test_int8_serialization(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit.
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname)
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map="auto")
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(
                input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
            )

tests/quantization/bnb/test_mixed_int8.py:401: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[-41,  25,   2,  ...,  14, -26, -29],
        [-52,  20,  10,  ..., -21,  -5,  15],
        [ 34,   9,  -3,  ....-8,  24,  ..., -25,  19,  46],
        [ 24,  44,  61,  ..., -23, -42, -34]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
_______________ MixedInt8Test.test_int8_serialization_regression _______________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_int8_serialization_regression>

    def test_int8_serialization_regression(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit - using not safetensors
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname, safe_serialization=False)
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map="auto")
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(
                input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
            )

tests/quantization/bnb/test_mixed_int8.py:428: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[-41,  25,   2,  ...,  14, -26, -29],
        [-52,  20,  10,  ..., -21,  -5,  15],
        [ 34,   9,  -3,  ....-8,  24,  ..., -25,  19,  46],
        [ 24,  44,  61,  ..., -23, -42, -34]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
________________ MixedInt8Test.test_int8_serialization_sharded _________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_int8_serialization_sharded>

    def test_int8_serialization_sharded(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit - sharded version.
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname, max_shard_size="200MB")
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname)
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(
                input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
            )

tests/quantization/bnb/test_mixed_int8.py:455: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[  0, -42,   8,  ...,  -4,  18,  -7],
        [ -3, -29,  22,  ...,  -9,  45,  10],
        [ -2,  16, -44,  ...,  -9, -51, -25],
        [  4,  11,  19,  ...,  -4,   5, -15]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ....16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:05,  1.38it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:02,  2.29it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:01,  2.96it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:01<00:01,  3.40it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:01<00:00,  3.71it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  3.94it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:02<00:00,  4.11it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  4.51it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  3.55it/s]
_______________ MixedInt8T5Test.test_inference_with_keep_in_fp32 _______________

self = <bnb.test_mixed_int8.MixedInt8T5Test testMethod=test_inference_with_keep_in_fp32>

    def test_inference_with_keep_in_fp32(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
    
        from transformers import T5ForConditionalGeneration
    
        # test with `google-t5/t5-small`
        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")
    
        # there was a bug with decoders - this test checks that it is fixed
        self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))
    
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt").to(torch_device)
>       _ = model.generate(**encoded_input)

tests/quantization/bnb/test_mixed_int8.py:543: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1741: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
src/transformers/generation/utils.py:549: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/t5/modeling_t5.py:1106: in forward
    layer_outputs = layer_module(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/t5/modeling_t5.py:686: in forward
    self_attention_outputs = self.layer[0](
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/t5/modeling_t5.py:593: in forward
    attention_output = self.SelfAttention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/t5/modeling_t5.py:512: in forward
    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
   ... ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
   ... ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
SA = (torch.Size([12, 512]), 'col'), SB = (torch.Size([512, 512]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
   ......, 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((12, 512), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([12, 512]), B: torch.Size([512, 512]), C: (12, 512); (lda, ldb, ldc): (c_int(12), c_int(512), c_int(12)); (m, n, k): (c_int(12), c_int(512), c_int(512))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
_________ MixedInt8T5Test.test_inference_with_keep_in_fp32_serialized __________

self = <bnb.test_mixed_int8.MixedInt8T5Test testMethod=test_inference_with_keep_in_fp32_serialized>

    def test_inference_with_keep_in_fp32_serialized(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly on
        a serialized model.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
    
        from transformers import T5ForConditionalGeneration
    
        # test with `google-t5/t5-small`
        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")
    
        with tempfile.TemporaryDirectory() as tmp_dir:
            model.save_pretrained(tmp_dir)
    
            model = T5ForConditionalGeneration.from_pretrained(tmp_dir)
    
            # there was a bug with decoders - this test checks that it is fixed
            self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))
    
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt").to(torch_device)
>           _ = model.generate(**encoded_input)

tests/quantization/bnb/test_mixed_int8.py:574: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1741: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
src/transformers/generation/utils.py:549: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/t5/modeling_t5.py:1106: in forward
    layer_outputs = layer_module(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/t5/modeling_t5.py:686: in forward
    self_attention_outputs = self.layer[0](
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/t5/modeling_t5.py:593: in forward
    attention_output = self.SelfAttention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/t5/modeling_t5.py:512: in forward
    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[-20, -21, -21,  ...,  -1,  52,  21],
        [ 22,  40,   3,  ...,  67,  -9, -44],
        [ 83,  55, -53,  ....10,  -6,  ...,  30, -44, -50],
        [ 29, -22, -15,  ...,  46,   4,   1]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[ -13,  -36,   49,  ...,  -83,    2,  -80],
        [ -59,  -22,  -52,  ...,   10,   14,    0],
        [   3,... ...,  -92,   26,  -32],
        [ -95,   64,  -39,  ...,   37,    1,  -39]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([12, 512]), 'col'), SB = (torch.Size([512, 512]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
   ......, 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((12, 512), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([12, 512]), B: torch.Size([512, 512]), C: (12, 512); (lda, ldb, ldc): (c_int(12), c_int(512), c_int(12)); (m, n, k): (c_int(12), c_int(512), c_int(512))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
_____________ MixedInt8T5Test.test_inference_without_keep_in_fp32 ______________

self = <bnb.test_mixed_int8.MixedInt8T5Test testMethod=test_inference_without_keep_in_fp32>

    def test_inference_without_keep_in_fp32(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
        from transformers import T5ForConditionalGeneration
    
        modules = T5ForConditionalGeneration._keep_in_fp32_modules
        T5ForConditionalGeneration._keep_in_fp32_modules = None
    
        # test with `google-t5/t5-small`
        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt").to(torch_device)
>       _ = model.generate(**encoded_input)

tests/quantization/bnb/test_mixed_int8.py:517: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1741: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
src/transformers/generation/utils.py:549: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/t5/modeling_t5.py:1106: in forward
    layer_outputs = layer_module(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/t5/modeling_t5.py:686: in forward
    self_attention_outputs = self.layer[0](
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/t5/modeling_t5.py:593: in forward
    attention_output = self.SelfAttention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/t5/modeling_t5.py:512: in forward
    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
   ... ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
   ... ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
SA = (torch.Size([12, 512]), 'col'), SB = (torch.Size([512, 512]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
   ......, 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((12, 512), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([12, 512]), B: torch.Size([512, 512]), C: (12, 512); (lda, ldb, ldc): (c_int(12), c_int(512), c_int(12)); (m, n, k): (c_int(12), c_int(512), c_int(512))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
_____________________ MixedInt8TestPipeline.test_pipeline ______________________

self = <bnb.test_mixed_int8.MixedInt8TestPipeline testMethod=test_pipeline>

    def test_pipeline(self):
        r"""
        The aim of this test is to verify that the mixed int8 is compatible with `pipeline` from transformers. Since
        we used pipline for inference speed benchmarking we want to make sure that this feature does not break anything
        on pipline.
        """
        # self._clear_cuda_cache()
        self.pipe = pipeline(
            "text-generation",
            model=self.model_name,
            model_kwargs={"device_map": "auto", "load_in_8bit": True},
            max_new_tokens=self.MAX_NEW_TOKENS,
        )
    
        # Real second forward pass
>       pipeline_output = self.pipe(self.input_text)

tests/quantization/bnb/test_mixed_int8.py:665: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/pipelines/text_generation.py:262: in __call__
    return super().__call__(text_inputs, **kwargs)
src/transformers/pipelines/base.py:1263: in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
src/transformers/pipelines/base.py:1270: in run_single
    model_outputs = self.forward(model_inputs, **forward_params)
src/transformers/pipelines/base.py:1170: in forward
    model_outputs = self._forward(model_inputs, **forward_params)
src/transformers/pipelines/text_generation.py:351: in _forward
    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[-41,  25,   2,  ...,  14, -26, -29],
        [-52,  20,  10,  ..., -21,  -5,  15],
        [ 34,   9,  -3,  ....-8,  24,  ..., -25,  19,  46],
        [ 24,  44,  61,  ..., -23, -42, -34]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
_________________ MixedInt8TestMultiGpu.test_multi_gpu_loading _________________

self = <bnb.test_mixed_int8.MixedInt8TestMultiGpu testMethod=test_multi_gpu_loading>

    def test_multi_gpu_loading(self):
        r"""
        This tests that the model has been loaded and can be used correctly on a multi-GPU setup.
        Let's just try to load a model on 2 GPUs and see if it works. The model we test has ~2GB of total, 3GB should suffice
        """
    
        model_parallel = AutoModelForCausalLM.from_pretrained(
            self.model_name, load_in_8bit=True, device_map="balanced"
        )
    
        # Check correct device map
        self.assertEqual(set(model_parallel.hf_device_map.values()), {0, 1})
    
        # Check that inference pass works on the model
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
    
        # Second real batch
>       output_parallel = model_parallel.generate(
            input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
        )

tests/quantization/bnb/test_mixed_int8.py:692: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[-41,  25,   2,  ...,  14, -26, -29],
        [-52,  20,  10,  ..., -21,  -5,  15],
        [ 34,   9,  -3,  ....-8,  24,  ..., -25,  19,  46],
        [ 24,  44,  61,  ..., -23, -42, -34]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
_______ MixedInt8TestCpuGpu.test_cpu_gpu_disk_loading_custom_device_map ________

self = <bnb.test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_disk_loading_custom_device_map>

    def test_cpu_gpu_disk_loading_custom_device_map(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.
        This time we also add `disk` on the device_map.
        """
        device_map = {
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": "cpu",
            "lm_head": 0,
            "transformer.h": 1,
            "transformer.ln_f": "disk",
        }
        bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_8bit=True)
        with tempfile.TemporaryDirectory() as tmpdirname:
            # Load model
            model_8bit = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=device_map,
                quantization_config=bnb_config,
                offload_folder=tmpdirname,
            )
    
            # Check that the model has been correctly set on device 0, 1, and `cpu`.
            self.assertEqual(set(model_8bit.hf_device_map.values()), {0, 1, "cpu", "disk"})
    
>           self.check_inference_correctness(model_8bit)

tests/quantization/bnb/test_mixed_int8.py:815: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/quantization/bnb/test_mixed_int8.py:709: in check_inference_correctness
    output_parallel = model.generate(input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[  0, -42,   8,  ...,  -4,  18,  -7],
        [ -3, -29,  22,  ...,  -9,  45,  10],
        [ -2,  16, -44,  ...,  -9, -51, -25],
        [  4,  11,  19,  ...,  -4,   5, -15]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ....16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu and disk.
____ MixedInt8TestCpuGpu.test_cpu_gpu_disk_loading_custom_device_map_kwargs ____

self = <bnb.test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_disk_loading_custom_device_map_kwargs>

    def test_cpu_gpu_disk_loading_custom_device_map_kwargs(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.
        This time we also add `disk` on the device_map - using the kwargs directly instead of the quantization config
        """
        device_map = {
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": "cpu",
            "lm_head": 0,
            "transformer.h": 1,
            "transformer.ln_f": "disk",
        }
        with tempfile.TemporaryDirectory() as tmpdirname:
            # Load model
            model_8bit = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=device_map,
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True,
                offload_folder=tmpdirname,
            )
    
            # Check that the model has been correctly set on device 0, 1, and `cpu`.
            self.assertEqual(set(model_8bit.hf_device_map.values()), {0, 1, "cpu", "disk"})
    
>           self.check_inference_correctness(model_8bit)

tests/quantization/bnb/test_mixed_int8.py:842: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/quantization/bnb/test_mixed_int8.py:709: in check_inference_correctness
    output_parallel = model.generate(input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[  0, -42,   8,  ...,  -4,  18,  -7],
        [ -3, -29,  22,  ...,  -9,  45,  10],
        [ -2,  16, -44,  ...,  -9, -51, -25],
        [  4,  11,  19,  ...,  -4,   5, -15]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ....16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu and disk.
__________ MixedInt8TestCpuGpu.test_cpu_gpu_loading_custom_device_map __________

self = <bnb.test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_loading_custom_device_map>

    def test_cpu_gpu_loading_custom_device_map(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.
        This time the device map is more organized than the test above and uses the abstraction
        `transformer.h` to encapsulate all the decoder layers.
        """
        device_map = {
            "transformer.word_embeddings": "cpu",
            "transformer.word_embeddings_layernorm": "cpu",
            "lm_head": "cpu",
            "transformer.h": 0,
            "transformer.ln_f": 1,
        }
        bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_8bit=True)
    
        # Load model
        model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map=device_map,
            quantization_config=bnb_config,
        )
    
        # Check that the model has been correctly set on device 0, 1, and `cpu`.
        self.assertEqual(set(model_8bit.hf_device_map.values()), {0, 1, "cpu"})
    
>       self.check_inference_correctness(model_8bit)

tests/quantization/bnb/test_mixed_int8.py:788: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/quantization/bnb/test_mixed_int8.py:709: in check_inference_correctness
    output_parallel = model.generate(input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[-25, -33,  11,  ...,  34,  22,  36]], device='cuda:0',
       dtype=torch.int8)
B = tensor([[ 23,  55, 109,  ...,  -6, -32,  -9],
        [-18, -39,  35,  ...,  26,  -5, -54],
        [ 39, -10, -52,  ....16,  51,  ...,  33,   9,   7],
        [ 53, -43,  13,  ..., -16, -34, -26]], device='cuda:0',
       dtype=torch.int8)
SA = (torch.Size([1, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)
Sout = ((1, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1, 2048]), B: torch.Size([6144, 2048]), C: (1, 6144); (lda, ldb, ldc): (c_int(1), c_int(6144), c_int(1)); (m, n, k): (c_int(1), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
__________ MixedInt8TestCpuGpu.test_cpu_gpu_loading_random_device_map __________

self = <bnb.test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_loading_random_device_map>

    def test_cpu_gpu_loading_random_device_map(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a random `device_map`.
        """
        device_map = {
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": 0,
            "lm_head": 0,
            "transformer.h.0": "cpu",
            "transformer.h.1": "cpu",
            "transformer.h.2": 0,
            "transformer.h.3": 0,
            "transformer.h.4": 0,
            "transformer.h.5": 0,
            "transformer.h.6": 0,
            "transformer.h.7": 0,
            "transformer.h.8": 0,
            "transformer.h.9": 1,
            "transformer.h.10": 0,
            "transformer.h.11": 1,
            "transformer.h.12": 0,
            "transformer.h.13": 0,
            "transformer.h.14": 1,
            "transformer.h.15": 0,
            "transformer.h.16": 0,
            "transformer.h.17": 1,
            "transformer.h.18": 1,
            "transformer.h.19": 0,
            "transformer.h.20": 1,
            "transformer.h.21": 1,
            "transformer.h.22": 0,
            "transformer.h.23": 0,
            "transformer.ln_f": 1,
        }
    
        bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_8bit=True)
    
        model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map=device_map,
            quantization_config=bnb_config,
        )
    
        # Check that the model has been correctly set on device 0, 1, and `cpu`.
        self.assertEqual(set(model_8bit.hf_device_map.values()), {0, 1, "cpu"})
    
>       self.check_inference_correctness(model_8bit)

tests/quantization/bnb/test_mixed_int8.py:761: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/quantization/bnb/test_mixed_int8.py:709: in check_inference_correctness
    output_parallel = model.generate(input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[-69,  27,  15,  ..., -11, -72, -19],
        [-50, -59,  22,  ...,  36, -45,  41],
        [ -6,  16,  11,  ....75,  -1,  ...,  21, -36,  24],
        [ 56,  59,  30,  ...,  36, -51,  33]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 2048]), 'col'), SB = (torch.Size([6144, 2048]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 6144), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 2048]), B: torch.Size([6144, 2048]), C: (4, 6144); (lda, ldb, ldc): (c_int(4), c_int(6144), c_int(4)); (m, n, k): (c_int(4), c_int(6144), c_int(2048))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device device because they were offloaded to the cpu.
_____________________ MixedInt8TestTraining.test_training ______________________

self = <bnb.test_mixed_int8.MixedInt8TestTraining testMethod=test_training>

    def test_training(self):
        if version.parse(importlib.metadata.version("bitsandbytes")) < version.parse("0.37.0"):
            self.skipTest(reason="This test requires bitsandbytes>=0.37.0")
    
        # Step 1: freeze all parameters
        model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)
    
        if torch.cuda.is_available():
            self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})
        else:
            self.assertTrue(all(param.device.type == "cpu" for param in model.parameters()))
    
        for param in model.parameters():
            param.requires_grad = False  # freeze the model - train adapters later
            if param.ndim == 1:
                # cast the small parameters (e.g. layernorm) to fp32 for stability
                param.data = param.data.to(torch.float32)
    
        # Step 2: add adapters
        for _, module in model.named_modules():
            if "OPTAttention" in repr(type(module)):
                module.q_proj = LoRALayer(module.q_proj, rank=16)
                module.k_proj = LoRALayer(module.k_proj, rank=16)
                module.v_proj = LoRALayer(module.v_proj, rank=16)
    
        # Step 3: dummy batch
        batch = self.tokenizer("Test batch ", return_tensors="pt").to(torch_device)
    
        # Step 4: Check if the gradient is not None
        with torch.autocast(torch_device):
>           out = model.forward(**batch)

tests/quantization/bnb/test_mixed_int8.py:881: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/opt/modeling_opt.py:1011: in forward
    outputs = self.model.decoder(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/opt/modeling_opt.py:729: in forward
    inputs_embeds = self.project_in(inputs_embeds)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[-43, -30, -60,  ..., -22,  -4,  32],
        [ 65,   9,  45,  ...,  29,  66, -59],
        [-17, -17,  35,  ..., -11, -64, -55],
        [-23,   5, -40,  ..., -74, -11,  -8]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[114, -71, -27,  ..., -73, -47,  66],
        [ 70,  95, -20,  ...,  15,  47, -15],
        [-86,  -4,   8,  ....11, -47,  ..., -43, -13, -47],
        [ 57, -15, -66,  ...,  42,  40, -64]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 512]), 'col'), SB = (torch.Size([1024, 512]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 1024), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 512]), B: torch.Size([1024, 512]), C: (4, 1024); (lda, ldb, ldc): (c_int(4), c_int(1024), c_int(4)); (m, n, k): (c_int(4), c_int(1024), c_int(512))
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
_________________ MixedInt8GPT2Test.test_fp32_int8_conversion __________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_fp32_int8_conversion>

    def test_fp32_int8_conversion(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        """
        model = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-small", load_in_8bit=True, device_map="auto")
>       self.assertTrue(model.decoder.block[0].layer[2].DenseReluDense.wo.weight.dtype == torch.float32)
E       AssertionError: False is not true

tests/quantization/bnb/test_mixed_int8.py:378: AssertionError
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
___________________ MixedInt8GPT2Test.test_generate_quality ____________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_generate_quality>

    def test_generate_quality(self):
        r"""
        Test the generation quality of the quantized model and see that we are matching the expected output.
        Given that we are operating on small numbers + the testing model is relatively small, we might not get
        the same output across GPUs. So we'll generate few tokens (5-10) and check their output.
        """
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = self.model_8bit.generate(
            input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
        )

tests/quantization/bnb/test_mixed_int8.py:273: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[  6, -38,   8,  ..., -56,  14,  11],
        [ 18,  17,  73,  ...,  43,   1, -10],
        [-22, -90,  37,  .... 9,  14,  ...,  30,   0,   7],
        [-30,  26,  56,  ..., -30, -55, -83]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 1600]), B: torch.Size([4800, 1600]), C: (4, 4800); (lda, ldb, ldc): (c_int(4), c_int(4800), c_int(4)); (m, n, k): (c_int(4), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
________________ MixedInt8GPT2Test.test_generate_quality_config ________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_generate_quality_config>

    def test_generate_quality_config(self):
        r"""
        Test that loading the model with the config is equivalent
        """
        bnb_config = BitsAndBytesConfig()
        bnb_config.load_in_8bit = True
    
        model_8bit_from_config = AutoModelForCausalLM.from_pretrained(
            self.model_name, quantization_config=bnb_config, device_map="auto"
        )
    
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model_8bit_from_config.generate(
            input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
        )

tests/quantization/bnb/test_mixed_int8.py:291: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[  6, -38,   8,  ..., -56,  14,  11],
        [ 18,  17,  73,  ...,  43,   1, -10],
        [-22, -90,  37,  .... 9,  14,  ...,  30,   0,   7],
        [-30,  26,  56,  ..., -30, -55, -83]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 1600]), B: torch.Size([4800, 1600]), C: (4, 4800); (lda, ldb, ldc): (c_int(4), c_int(4800), c_int(4)); (m, n, k): (c_int(4), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
______________ MixedInt8GPT2Test.test_generate_quality_dequantize ______________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_generate_quality_dequantize>

    def test_generate_quality_dequantize(self):
        r"""
        Test that loading the model and dequantizing it produce correct results
        """
        bnb_config = BitsAndBytesConfig(load_in_8bit=True)
    
        model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name, quantization_config=bnb_config, device_map="auto"
        )
    
>       model_8bit.dequantize()

tests/quantization/bnb/test_mixed_int8.py:307: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/modeling_utils.py:1420: in dequantize
    return hf_quantizer.dequantize(self)
src/transformers/quantizers/base.py:202: in dequantize
    model = self._dequantize(model)
src/transformers/quantizers/quantizer_bnb_8bit.py:313: in _dequantize
    model = dequantize_and_replace(
src/transformers/integrations/bitsandbytes.py:460: in dequantize_and_replace
    model, has_been_replaced = _dequantize_and_replace(
src/transformers/integrations/bitsandbytes.py:442: in _dequantize_and_replace
    _, has_been_replaced = _dequantize_and_replace(
src/transformers/integrations/bitsandbytes.py:442: in _dequantize_and_replace
    _, has_been_replaced = _dequantize_and_replace(
src/transformers/integrations/bitsandbytes.py:442: in _dequantize_and_replace
    _, has_been_replaced = _dequantize_and_replace(
src/transformers/integrations/bitsandbytes.py:442: in _dequantize_and_replace
    _, has_been_replaced = _dequantize_and_replace(
src/transformers/integrations/bitsandbytes.py:426: in _dequantize_and_replace
    new_module.weight = torch.nn.Parameter(dequantize_bnb_weight(module.weight, dtype, state))
src/transformers/integrations/bitsandbytes.py:363: in dequantize_bnb_weight
    out32, Sout32 = bnb.functional.igemmlt(im, state.CxB, Sim, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[127,   0,   0,  ...,   0,   0,   0],
        [  0, 127,   0,  ...,   0,   0,   0],
        [  0,   0, 127,  .... 0,   0,  ...,   0, 127,   0],
        [  0,   0,   0,  ...,   0,   0, 127]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[  6, -38,   8,  ..., -56,  14,  11],
        [ 18,  17,  73,  ...,  43,   1, -10],
        [-22, -90,  37,  .... 9,  14,  ...,  30,   0,   7],
        [-30,  26,  56,  ..., -30, -55, -83]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([1600, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
   ......, 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((1600, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([1600, 1600]), B: torch.Size([4800, 1600]), C: (1600, 4800); (lda, ldb, ldc): (c_int(1600), c_int(4800), c_int(1600)); (m, n, k): (c_int(1600), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
_________________ MixedInt8GPT2Test.test_int8_from_pretrained __________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_from_pretrained>

    def test_int8_from_pretrained(self):
        r"""
        Test whether loading a 8bit model from the Hub works as expected
        """
        from bitsandbytes.nn import Int8Params
    
        model_id = "ybelkada/gpt2-xl-8bit"
    
        model = AutoModelForCausalLM.from_pretrained(model_id)
    
        linear = get_some_linear_layer(model)
        self.assertTrue(linear.weight.__class__ == Int8Params)
        self.assertTrue(hasattr(linear.weight, "SCB"))
    
        # generate
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model.generate(input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10)

tests/quantization/bnb/test_mixed_int8.py:921: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[ 34, -19, -43,  ...,  -2,  16,   8],
        [ 20,   1,  16,  ...,   9, -20,   3],
        [ -7,  -7, -18,  ..., -80,  22, -95],
        [  4,  -5,  11,  ...,  -8,  20,  -6]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ....24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 1600]), B: torch.Size([4800, 1600]), C: (4, 4800); (lda, ldb, ldc): (c_int(4), c_int(4800), c_int(4)); (m, n, k): (c_int(4), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
`low_cpu_mem_usage` was None, now set to True since model is quantized.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
__________________ MixedInt8GPT2Test.test_int8_serialization ___________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_serialization>

    def test_int8_serialization(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit.
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname)
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map="auto")
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(
                input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
            )

tests/quantization/bnb/test_mixed_int8.py:401: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[  6, -38,   8,  ..., -56,  14,  11],
        [ 18,  17,  73,  ...,  43,   1, -10],
        [-22, -90,  37,  .... 9,  14,  ...,  30,   0,   7],
        [-30,  26,  56,  ..., -30, -55, -83]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 1600]), B: torch.Size([4800, 1600]), C: (4, 4800); (lda, ldb, ldc): (c_int(4), c_int(4800), c_int(4)); (m, n, k): (c_int(4), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
_____________ MixedInt8GPT2Test.test_int8_serialization_regression _____________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_serialization_regression>

    def test_int8_serialization_regression(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit - using not safetensors
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname, safe_serialization=False)
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map="auto")
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(
                input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
            )

tests/quantization/bnb/test_mixed_int8.py:428: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int8)
B = tensor([[  6, -38,   8,  ..., -56,  14,  11],
        [ 18,  17,  73,  ...,  43,   1, -10],
        [-22, -90,  37,  .... 9,  14,  ...,  30,   0,   7],
        [-30,  26,  56,  ..., -30, -55, -83]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 1600]), B: torch.Size([4800, 1600]), C: (4, 4800); (lda, ldb, ldc): (c_int(4), c_int(4800), c_int(4)); (m, n, k): (c_int(4), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
______________ MixedInt8GPT2Test.test_int8_serialization_sharded _______________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_serialization_sharded>

    def test_int8_serialization_sharded(self):
        r"""
        Test whether it is possible to serialize a model in 8-bit - sharded version.
        """
        from bitsandbytes.nn import Int8Params
    
        with tempfile.TemporaryDirectory() as tmpdirname:
            self.model_8bit.save_pretrained(tmpdirname, max_shard_size="200MB")
    
            # check that the file `quantization_config` is present
            config = AutoConfig.from_pretrained(tmpdirname)
            self.assertTrue(hasattr(config, "quantization_config"))
    
            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname)
    
            linear = get_some_linear_layer(model_from_saved)
            self.assertTrue(linear.weight.__class__ == Int8Params)
            self.assertTrue(hasattr(linear.weight, "SCB"))
    
            # generate
            encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>           output_sequences = model_from_saved.generate(
                input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
            )

tests/quantization/bnb/test_mixed_int8.py:455: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/utils/_contextlib.py:116: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:1996: in generate
    result = self._sample(
src/transformers/generation/utils.py:2923: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1315: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:1129: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:614: in forward
    attn_outputs = self.attn(
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/gpt2/modeling_gpt2.py:517: in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/nn/modules/module.py:1562: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/accelerate/hooks.py:169: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:842: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.8/site-packages/torch/autograd/function.py:574: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:406: in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
../bnb/bitsandbytes/functional.py:1744: in igemmlt
    return backends[A.device.type].igemmlt(A, B, SA, SB, out=out, Sout=Sout, dtype=dtype)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bitsandbytes.backends.rocm.ROCmBackend object at 0x7fa3a1744940>
A = tensor([[ 34, -19, -43,  ...,  -2,  16,   8],
        [ 20,   1,  16,  ...,   9, -20,   3],
        [ -7,  -7, -18,  ..., -80,  22, -95],
        [  4,  -5,  11,  ...,  -8,  20,  -6]], device='cuda:1',
       dtype=torch.int8)
B = tensor([[-23,  -6,  -3,  ...,  17,  -2,  49],
        [-22, -34,   5,  ...,  66,  -5,  58],
        [ 14, -32,   1,  ....24,   6,  ..., -13, -33, -64],
        [ -8,  28,   5,  ...,  -4,  47,  83]], device='cuda:1',
       dtype=torch.int8)
SA = (torch.Size([4, 1600]), 'col'), SB = (torch.Size([4800, 1600]), 'col')
out = tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1', dtype=torch.int32)
Sout = ((4, 4800), 'col'), dtype = torch.int32

    def igemmlt(
        self,
        A: torch.Tensor,
        B: torch.Tensor,
        SA: Tuple[torch.Size, str],
        SB: Tuple[torch.Size, str],
        out: Optional[torch.Tensor] = None,
        Sout: Optional[Tuple[torch.Size, str]] = None,
        dtype=torch.int32,
    ):
        shapeA = SA[0]
        shapeB = SB[0]
        dimsA = len(shapeA)
        dimsB = len(shapeB)
    
        assert dimsB == 2, "Only two dimensional matrices are supported for argument B"
        if dimsA == 2:
            m = shapeA[0]
        elif dimsA == 3:
            m = shapeA[0] * shapeA[1]
    
        rows = n = shapeB[0]
        assert prod(list(shapeA)) > 0, f"Input tensor dimensions need to be > 0: {shapeA}"
    
        # if the tensor is empty, return a transformed empty tensor with the right dimensions
        if shapeA[0] == 0 and dimsA == 2:
            return torch.empty((0, shapeB[0]), device=A.device, dtype=torch.float16)
        elif shapeA[1] == 0 and dimsA == 3:
            return torch.empty(tuple(shapeA[:2] + [shapeB[0]]), device=A.device, dtype=torch.float16)
    
        if dimsA == 2 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeB[0]), dtype, A.device, "col32", "row")
        elif dimsA == 3 and out is None:
            if HIP_ENVIRONMENT:
                # Use col format for HIP
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col", "row")
            else:
                out, Sout = get_transform_buffer((shapeA[0], shapeA[1], shapeB[0]), dtype, A.device, "col32", "row")
    
        assert dimsB != 3, "len(B.shape)==3 not supported"
        assert A.device.type == "cuda"
        assert B.device.type == "cuda"
        assert A.dtype == torch.int8
        assert B.dtype == torch.int8
        assert out.dtype == dtype
        if HIP_ENVIRONMENT:
            # Use col format for HIP
            assert SA[1] == "col"
            assert SB[1] == "col"
            assert Sout[1] == "col"
        else:
            assert SA[1] == "col32"
            assert SB[1] in ["col_turing", "col_ampere"]
            assert Sout[1] == "col32"
        assert (
            shapeA[-1] == shapeB[-1]
        ), f"Matmullt only supports A @ B^T. Inner matrix dimensions do not match: A @ B = {shapeA} @ {shapeB}"
    
        formatB = SB[1]
        prev_device = A.device
        torch.cuda.set_device(A.device)
    
        ptr = CUBLAS_Context.get_instance().get_context(A.device)
        ptrA = get_ptr(A)
        ptrB = get_ptr(B)
        ptrC = get_ptr(out)
    
        k = shapeA[-1]
        if HIP_ENVIRONMENT:
            # Set ld values for col format
            lda = ct.c_int32(m)
            ldb = ct.c_int32(shapeB[0])
            ldc = ct.c_int32(m)
        else:
            lda = ct.c_int32(m * 32)
            if formatB == "col_turing":
                # turing: tiles with rows filled up to multiple of 8 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 7) // 8) * 8 * 32)
            else:
                # ampere: tiles with rows filled up to multiple of 32 rows by 32 columns
                # n = rows
                ldb = ct.c_int32(((rows + 31) // 32) * 32 * 32)
    
            ldc = ct.c_int32(m * 32)
        m = ct.c_int32(m)
        n = ct.c_int32(n)
        k = ct.c_int32(k)
    
        has_error = 0
        ptrRowScale = get_ptr(None)
        is_on_gpu([A, B, out])
    
        if formatB == "col_turing" or HIP_ENVIRONMENT:
            if dtype == torch.int32:
                has_error = lib.cigemmlt_turing_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_turing_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        elif formatB == "col_ampere":
            if dtype == torch.int32:
                has_error = lib.cigemmlt_ampere_32(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
            else:
                has_error = lib.cigemmlt_ampere_8(ptr, m, n, k, ptrA, ptrB, ptrC, ptrRowScale, lda, ldb, ldc)
    
        if has_error == 100:  # `ERR_NOT_IMPLEMENTED` is defined as 100 in `ops.cu`, `ops.hip`
            raise NotImplementedError("igemmlt not available (probably built with NO_CUBLASLT)")
    
        if has_error:
            print(
                f"A: {shapeA}, B: {shapeB}, C: {Sout[0]}; (lda, ldb, ldc): {(lda, ldb, ldc)}; (m, n, k): {(m, n, k)}"
            )
>           raise Exception("cublasLt ran into an error!")
E           Exception: cublasLt ran into an error!

../bnb/bitsandbytes/backends/cuda.py:360: Exception
----------------------------- Captured stdout call -----------------------------
A: torch.Size([4, 1600]), B: torch.Size([4800, 1600]), C: (4, 4800); (lda, ldb, ldc): (c_int(4), c_int(4800), c_int(4)); (m, n, k): (c_int(4), c_int(4800), c_int(1600))
----------------------------- Captured stderr call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:00<00:02,  3.30it/s]Loading checkpoint shards:  22%|██▏       | 2/9 [00:00<00:02,  3.30it/s]Loading checkpoint shards:  33%|███▎      | 3/9 [00:00<00:01,  3.44it/s]Loading checkpoint shards:  44%|████▍     | 4/9 [00:01<00:01,  3.14it/s]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:01<00:01,  3.25it/s]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:01<00:00,  3.35it/s]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:02<00:00,  3.42it/s]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:02<00:00,  3.30it/s]Loading checkpoint shards: 100%|██████████| 9/9 [00:02<00:00,  3.54it/s]Loading checkpoint shards: 100%|██████████| 9/9 [00:02<00:00,  3.39it/s]
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
=============================== warnings summary ===============================
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert_trust_remote_code
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_get_keys_to_not_convert_trust_remote_code
  /home/huggingface/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7/configuration_mpt.py:90: DeprecationWarning: verbose argument for MPTConfig is now ignored and will be removed. Use python_log_level instead.
    warnings.warn(DeprecationWarning('verbose argument for MPTConfig is now ignored and will be removed. Use python_log_level instead.'))

tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert_trust_remote_code
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_get_keys_to_not_convert_trust_remote_code
  /home/huggingface/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7/configuration_mpt.py:97: UserWarning: alibi is turned on, setting `learned_pos_emb` to `False.`
    warnings.warn(f'alibi is turned on, setting `learned_pos_emb` to `False.`')

tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_int8_serialization
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_int8_serialization_regression
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization_regression
  /home/huggingface/src/transformers/src/transformers/quantizers/auto.py:178: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
    warnings.warn(warning_msg)

tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_loading_custom_device_map
  /home/huggingface/src/transformers/src/transformers/generation/utils.py:1881: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on meta. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('meta') before running `.generate()`.
    warnings.warn(

tests/quantization/bnb/test_mixed_int8.py: 18 warnings
  /home/huggingface/src/transformers/src/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
============ 25 failed, 20 passed, 27 warnings in 365.57s (0:06:05) ============
hipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectederror detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detectedhipBLAS API failed with status 6
error detected