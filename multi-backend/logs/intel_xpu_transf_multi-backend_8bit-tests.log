============================= test session starts ==============================
platform linux -- Python 3.10.14, pytest-8.2.2, pluggy-1.5.0 -- /home/sdp/.condax/mamba/envs/bnb/bin/python3.10
cachedir: .pytest_cache
rootdir: /home/sdp/src/transformers
configfile: pyproject.toml
collecting ... collected 45 items

tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_device_and_dtype_assignment FAILED [  2%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_fp32_int8_conversion PASSED [  4%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_generate_quality FAILED [  6%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_generate_quality_config FAILED [  8%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_generate_quality_dequantize FAILED [ 11%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert PASSED [ 13%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert_trust_remote_code PASSED [ 15%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_int8_from_pretrained FAILED [ 17%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_int8_serialization FAILED [ 20%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_int8_serialization_regression FAILED [ 22%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_int8_serialization_sharded FAILED [ 24%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_linear_are_8bit FAILED [ 26%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_llm_skip FAILED [ 28%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_memory_footprint FAILED [ 31%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_original_dtype FAILED [ 33%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_quantization_config_json_serialization FAILED [ 35%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_raise_if_config_and_load_in_8bit FAILED [ 37%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8T5Test::test_inference_with_keep_in_fp32 FAILED [ 40%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8T5Test::test_inference_with_keep_in_fp32_serialized FAILED [ 42%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8T5Test::test_inference_without_keep_in_fp32 FAILED [ 44%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8ModelClassesTest::test_correct_head_class FAILED [ 46%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestPipeline::test_pipeline FAILED [ 48%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestPipeline::test_pipeline ERROR [ 48%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestMultiGpu::test_multi_gpu_loading FAILED [ 51%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_disk_loading_custom_device_map FAILED [ 53%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_disk_loading_custom_device_map_kwargs FAILED [ 55%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_loading_custom_device_map FAILED [ 57%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestCpuGpu::test_cpu_gpu_loading_random_device_map FAILED [ 60%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestTraining::test_training FAILED [ 62%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_device_and_dtype_assignment FAILED [ 64%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_fp32_int8_conversion FAILED [ 66%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_generate_quality FAILED [ 68%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_generate_quality_config FAILED [ 71%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_generate_quality_dequantize FAILED [ 73%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_get_keys_to_not_convert FAILED [ 75%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_get_keys_to_not_convert_trust_remote_code FAILED [ 77%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_from_pretrained FAILED [ 80%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization FAILED [ 82%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization_regression FAILED [ 84%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_int8_serialization_sharded FAILED [ 86%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_linear_are_8bit FAILED [ 88%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_llm_skip FAILED [ 91%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_memory_footprint FAILED [ 93%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_original_dtype FAILED [ 95%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_quantization_config_json_serialization FAILED [ 97%]
tests/quantization/bnb/test_mixed_int8.py::MixedInt8GPT2Test::test_raise_if_config_and_load_in_8bit FAILED [100%]

==================================== ERRORS ====================================
___________ ERROR at teardown of MixedInt8TestPipeline.test_pipeline ___________

self = <bnb.test_mixed_int8.MixedInt8TestPipeline testMethod=test_pipeline>

    def tearDown(self):
        r"""
        TearDown function needs to be called at the end of each test to free the GPU memory and cache, also to
        avoid unexpected behaviors. Please see: https://discuss.pytorch.org/t/how-can-we-release-gpu-memory-cache/14530/27
        """
>       del self.pipe
E       AttributeError: pipe

tests/quantization/bnb/test_mixed_int8.py:644: AttributeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=================================== FAILURES ===================================
________________ MixedInt8Test.test_device_and_dtype_assignment ________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_device_and_dtype_assignment>

    def test_device_and_dtype_assignment(self):
        r"""
        Test whether attempting to change the device or cast the dtype of a model
        after converting it to 8-bit precision will raise an appropriate error.
        The test ensures that such operations are prohibited on 8-bit models
        to prevent invalid conversions.
        """
        with self.assertRaises(ValueError):
            # Tries with `str`
            self.model_8bit.to("cpu")
    
        with self.assertRaises(ValueError):
            # Tries with a `dtype``
            self.model_8bit.to(torch.float16)
    
        with self.assertRaises(ValueError):
            # Tries with a `device`
            self.model_8bit.to(torch.device(torch_device))
    
        with self.assertRaises(ValueError):
            # Tries to cast the 8-bit model to float32 using `float()`
            self.model_8bit.float()
    
        with self.assertRaises(ValueError):
            # Tries to cast the 4-bit model to float16 using `half()`
            self.model_8bit.half()
    
        # Test if we did not break anything
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
    
        self.model_fp16 = self.model_fp16.to(torch.float32)
>       _ = self.model_fp16.generate(input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10)

tests/quantization/bnb/test_mixed_int8.py:362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/utils/_contextlib.py:115: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:2024: in generate
    result = self._sample(
src/transformers/generation/utils.py:2982: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:687: in forward
    inputs_embeds = self.word_embeddings(input_ids)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162: in forward
    return F.embedding(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[59414,  2670,  4040,   632]], device='xpu:0')
weight = Parameter containing:
tensor([[ 6.0310e-03, -1.2779e-03, -6.3667e-03,  ..., -8.1940e-03,
         -1.3077e-02,  9.6817...    [ 6.7663e-04,  5.4169e-04,  1.9703e-03,  ...,  1.3475e-03,
         -5.2929e-05, -8.6451e-04]], requires_grad=True)
padding_idx = -1, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False
sparse = False

    def embedding(
        input: Tensor,
        weight: Tensor,
        padding_idx: Optional[int] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2.0,
        scale_grad_by_freq: bool = False,
        sparse: bool = False,
    ) -> Tensor:
        r"""A simple lookup table that looks up embeddings in a fixed dictionary and size.
    
        This module is often used to retrieve word embeddings using indices.
        The input to the module is a list of indices, and the embedding matrix,
        and the output is the corresponding word embeddings.
    
        See :class:`torch.nn.Embedding` for more details.
    
        .. note::
            Note that the analytical gradients of this function with respect to
            entries in :attr:`weight` at the row specified by :attr:`padding_idx`
            are expected to differ from the numerical ones.
    
        .. note::
            Note that `:class:`torch.nn.Embedding` differs from this function in
            that it initializes the row of :attr:`weight` specified by
            :attr:`padding_idx` to all zeros on construction.
    
        Args:
            input (LongTensor): Tensor containing indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                         i.e. it remains as a fixed "pad".
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
            scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
    
        Shape:
            - Input: LongTensor of arbitrary shape containing the indices to extract
            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,
              where V = maximum index + 1 and embedding_dim = the embedding size
            - Output: `(*, embedding_dim)`, where `*` is the input shape
    
        Examples::
    
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])
            >>> # an embedding matrix containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> F.embedding(input, embedding_matrix)
            tensor([[[ 0.8490,  0.9625,  0.6753],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.6246,  0.9751,  0.3618],
                     [ 0.4161,  0.2419,  0.7383]],
    
                    [[ 0.6246,  0.9751,  0.3618],
                     [ 0.0237,  0.7794,  0.0528],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.3385,  0.8612,  0.1867]]])
    
            >>> # example with padding_idx
            >>> weights = torch.rand(10, 3)
            >>> weights[0, :].zero_()
            >>> embedding_matrix = weights
            >>> input = torch.tensor([[0, 2, 0, 5]])
            >>> F.embedding(input, embedding_matrix, padding_idx=0)
            tensor([[[ 0.0000,  0.0000,  0.0000],
                     [ 0.5609,  0.5384,  0.8720],
                     [ 0.0000,  0.0000,  0.0000],
                     [ 0.6262,  0.2438,  0.7471]]])
        """
    
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(
                embedding,
                (input, weight),
                input,
                weight,
                padding_idx=padding_idx,
                max_norm=max_norm,
                norm_type=norm_type,
                scale_grad_by_freq=scale_grad_by_freq,
                sparse=sparse,
            )
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < weight.size(0), "Padding_idx must be within num_embeddings"
            elif padding_idx < 0:
                assert padding_idx >= -weight.size(0), "Padding_idx must be within num_embeddings"
                padding_idx = weight.size(0) + padding_idx
        else:
            padding_idx = -1
        if max_norm is not None:
            # Note [embedding_renorm contiguous]
            # `embedding_renorm_` will call .contiguous() on input anyways, so we
            # call it here and take advantage of the improved locality in the
            # `embedding` call below too.
            input = input.contiguous()
            # Note [embedding_renorm set_grad_enabled]
            # XXX: equivalent to
            # with torch.no_grad():
            #   torch.embedding_renorm_
            # remove once script supports set_grad_enabled
            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
>       return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
E       RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and xpu:0! (when checking argument for argument index in method wrapper_XPU__index_select)

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/functional.py:2233: RuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
2024-08-18 23:06:57,071 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-08-18 23:06:57,072 - accelerate.big_modeling - WARNING - You shouldn't move a model that is dispatched using accelerate hooks.
2024-08-18 23:06:57,073 - accelerate.big_modeling - WARNING - You shouldn't move a model that is dispatched using accelerate hooks.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
WARNING  accelerate.big_modeling:big_modeling.py:451 You shouldn't move a model that is dispatched using accelerate hooks.
_____________________ MixedInt8Test.test_generate_quality ______________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_generate_quality>

    def test_generate_quality(self):
        r"""
        Test the generation quality of the quantized model and see that we are matching the expected output.
        Given that we are operating on small numbers + the testing model is relatively small, we might not get
        the same output across GPUs. So we'll generate few tokens (5-10) and check their output.
        """
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = self.model_8bit.generate(
            input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
        )

tests/quantization/bnb/test_mixed_int8.py:273: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/utils/_contextlib.py:115: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:2024: in generate
    result = self._sample(
src/transformers/generation/utils.py:2982: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:710: in forward
    hidden_states = self.word_embeddings_layernorm(inputs_embeds)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/normalization.py:196: in forward
    return F.layer_norm(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[-0.0004,  0.0034, -0.0008,  ..., -0.0143, -0.0035, -0.0107],
         [-0.0192, -0.0108,  0.0061,  ...,  0.0...-0.0068,  0.0030],
         [-0.0034,  0.0034,  0.0021,  ...,  0.0009,  0.0089, -0.0092]]],
       dtype=torch.float16)
normalized_shape = (2048,)
weight = Parameter containing:
tensor([0.5054, 0.4036, 0.5820,  ..., 0.3984, 0.4233, 0.4556],
       dtype=torch.float16, requires_grad=True)
bias = Parameter containing:
tensor([ 0.0114,  0.0330,  0.1769,  ...,  0.0318, -0.0189, -0.0546],
       dtype=torch.float16, requires_grad=True)
eps = 1e-05

    def layer_norm(
        input: Tensor,
        normalized_shape: List[int],
        weight: Optional[Tensor] = None,
        bias: Optional[Tensor] = None,
        eps: float = 1e-5,
    ) -> Tensor:
        r"""Applies Layer Normalization for last certain number of dimensions.
    
        See :class:`~torch.nn.LayerNorm` for details.
        """
        if has_torch_function_variadic(input, weight, bias):
            return handle_torch_function(
                layer_norm, (input, weight, bias), input, normalized_shape, weight=weight, bias=bias, eps=eps
            )
>       return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
E       RuntimeError: "LayerNormKernelImpl" not implemented for 'Half'

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/functional.py:2543: RuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
2024-08-18 23:07:01,658 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
__________________ MixedInt8Test.test_generate_quality_config __________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_generate_quality_config>

    def test_generate_quality_config(self):
        r"""
        Test that loading the model with the config is equivalent
        """
        bnb_config = BitsAndBytesConfig()
        bnb_config.load_in_8bit = True
    
        model_8bit_from_config = AutoModelForCausalLM.from_pretrained(
            self.model_name, quantization_config=bnb_config, device_map="auto"
        )
    
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model_8bit_from_config.generate(
            input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
        )

tests/quantization/bnb/test_mixed_int8.py:291: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/utils/_contextlib.py:115: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:2024: in generate
    result = self._sample(
src/transformers/generation/utils.py:2982: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:710: in forward
    hidden_states = self.word_embeddings_layernorm(inputs_embeds)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/normalization.py:196: in forward
    return F.layer_norm(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[-0.0004,  0.0034, -0.0008,  ..., -0.0143, -0.0035, -0.0107],
         [-0.0192, -0.0108,  0.0061,  ...,  0.0...-0.0068,  0.0030],
         [-0.0034,  0.0034,  0.0021,  ...,  0.0009,  0.0089, -0.0092]]],
       dtype=torch.float16)
normalized_shape = (2048,)
weight = Parameter containing:
tensor([0.5054, 0.4036, 0.5820,  ..., 0.3984, 0.4233, 0.4556],
       dtype=torch.float16, requires_grad=True)
bias = Parameter containing:
tensor([ 0.0114,  0.0330,  0.1769,  ...,  0.0318, -0.0189, -0.0546],
       dtype=torch.float16, requires_grad=True)
eps = 1e-05

    def layer_norm(
        input: Tensor,
        normalized_shape: List[int],
        weight: Optional[Tensor] = None,
        bias: Optional[Tensor] = None,
        eps: float = 1e-5,
    ) -> Tensor:
        r"""Applies Layer Normalization for last certain number of dimensions.
    
        See :class:`~torch.nn.LayerNorm` for details.
        """
        if has_torch_function_variadic(input, weight, bias):
            return handle_torch_function(
                layer_norm, (input, weight, bias), input, normalized_shape, weight=weight, bias=bias, eps=eps
            )
>       return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
E       RuntimeError: "LayerNormKernelImpl" not implemented for 'Half'

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/functional.py:2543: RuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
2024-08-18 23:07:03,323 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-08-18 23:07:03,936 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
________________ MixedInt8Test.test_generate_quality_dequantize ________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_generate_quality_dequantize>

    def test_generate_quality_dequantize(self):
        r"""
        Test that loading the model and dequantizing it produce correct results
        """
        bnb_config = BitsAndBytesConfig(load_in_8bit=True)
    
        model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name, quantization_config=bnb_config, device_map="auto"
        )
    
        model_8bit.dequantize()
    
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model_8bit.generate(
            input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10
        )

tests/quantization/bnb/test_mixed_int8.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/utils/_contextlib.py:115: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:2024: in generate
    result = self._sample(
src/transformers/generation/utils.py:2982: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:710: in forward
    hidden_states = self.word_embeddings_layernorm(inputs_embeds)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/normalization.py:196: in forward
    return F.layer_norm(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[-0.0004,  0.0034, -0.0008,  ..., -0.0143, -0.0035, -0.0107],
         [-0.0192, -0.0108,  0.0061,  ...,  0.0...-0.0068,  0.0030],
         [-0.0034,  0.0034,  0.0021,  ...,  0.0009,  0.0089, -0.0092]]],
       dtype=torch.float16)
normalized_shape = (2048,)
weight = Parameter containing:
tensor([0.5054, 0.4036, 0.5820,  ..., 0.3984, 0.4233, 0.4556],
       dtype=torch.float16, requires_grad=True)
bias = Parameter containing:
tensor([ 0.0114,  0.0330,  0.1769,  ...,  0.0318, -0.0189, -0.0546],
       dtype=torch.float16, requires_grad=True)
eps = 1e-05

    def layer_norm(
        input: Tensor,
        normalized_shape: List[int],
        weight: Optional[Tensor] = None,
        bias: Optional[Tensor] = None,
        eps: float = 1e-5,
    ) -> Tensor:
        r"""Applies Layer Normalization for last certain number of dimensions.
    
        See :class:`~torch.nn.LayerNorm` for details.
        """
        if has_torch_function_variadic(input, weight, bias):
            return handle_torch_function(
                layer_norm, (input, weight, bias), input, normalized_shape, weight=weight, bias=bias, eps=eps
            )
>       return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
E       RuntimeError: "LayerNormKernelImpl" not implemented for 'Half'

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/functional.py:2543: RuntimeError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
2024-08-18 23:07:05,790 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-08-18 23:07:06,534 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
For some reason the model has not been properly dequantized. You might see unexpected behavior.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
___________________ MixedInt8Test.test_int8_from_pretrained ____________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_int8_from_pretrained>

    def test_int8_from_pretrained(self):
        r"""
        Test whether loading a 8bit model from the Hub works as expected
        """
        from bitsandbytes.nn import Int8Params
    
        model_id = "ybelkada/bloom-1b7-8bit"
    
        model = AutoModelForCausalLM.from_pretrained(model_id)
    
        linear = get_some_linear_layer(model)
        self.assertTrue(linear.weight.__class__ == Int8Params)
        self.assertTrue(hasattr(linear.weight, "SCB"))
    
        # generate
        encoded_input = self.tokenizer(self.input_text, return_tensors="pt")
>       output_sequences = model.generate(input_ids=encoded_input["input_ids"].to(torch_device), max_new_tokens=10)

tests/quantization/bnb/test_mixed_int8.py:477: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/utils/_contextlib.py:115: in decorate_context
    return func(*args, **kwargs)
src/transformers/generation/utils.py:2024: in generate
    result = self._sample(
src/transformers/generation/utils.py:2982: in _sample
    outputs = self(**model_inputs, return_dict=True)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:955: in forward
    transformer_outputs = self.transformer(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:744: in forward
    outputs = block(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:457: in forward
    attn_outputs = self.self_attention(
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
src/transformers/models/bloom/modeling_bloom.py:319: in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../accelerate/src/accelerate/hooks.py:170: in new_forward
    output = module._old_forward(*args, **kwargs)
../bnb/bitsandbytes/nn/modules.py:838: in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
../bnb/bitsandbytes/autograd/_functions.py:567: in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/autograd/function.py:539: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
../bnb/bitsandbytes/autograd/_functions.py:299: in forward
    using_igemmlt = supports_igemmlt(A.device) and not state.force_no_igemmlt
../bnb/bitsandbytes/autograd/_functions.py:228: in supports_igemmlt
    if torch.cuda.get_device_capability(device=device) < (7, 5):
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:435: in get_device_capability
    prop = get_device_properties(device)
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:449: in get_device_properties
    _lazy_init()  # will define _get_device_properties
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
2024-08-18 23:07:18,846 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
Unused kwargs: ['_from_model_config', 'transformers_version']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
____________________ MixedInt8Test.test_int8_serialization _____________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_int8_serialization>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
2024-08-18 23:07:23,537 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
------------------------------ Captured log call -------------------------------
WARNING  accelerate.big_modeling:big_modeling.py:436 Some parameters are on the meta device because they were offloaded to the cpu.
_______________ MixedInt8Test.test_int8_serialization_regression _______________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_int8_serialization_regression>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
________________ MixedInt8Test.test_int8_serialization_sharded _________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_int8_serialization_sharded>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
______________________ MixedInt8Test.test_linear_are_8bit ______________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_linear_are_8bit>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_________________________ MixedInt8Test.test_llm_skip __________________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_llm_skip>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_____________________ MixedInt8Test.test_memory_footprint ______________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_memory_footprint>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
______________________ MixedInt8Test.test_original_dtype _______________________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_original_dtype>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
__________ MixedInt8Test.test_quantization_config_json_serialization ___________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_quantization_config_json_serialization>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_____________ MixedInt8Test.test_raise_if_config_and_load_in_8bit ______________

self = <bnb.test_mixed_int8.MixedInt8Test testMethod=test_raise_if_config_and_load_in_8bit>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_______________ MixedInt8T5Test.test_inference_with_keep_in_fp32 _______________

self = <bnb.test_mixed_int8.MixedInt8T5Test testMethod=test_inference_with_keep_in_fp32>

    def test_inference_with_keep_in_fp32(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
    
        from transformers import T5ForConditionalGeneration
    
        # test with `google-t5/t5-small`
>       model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:537: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_________ MixedInt8T5Test.test_inference_with_keep_in_fp32_serialized __________

self = <bnb.test_mixed_int8.MixedInt8T5Test testMethod=test_inference_with_keep_in_fp32_serialized>

    def test_inference_with_keep_in_fp32_serialized(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly on
        a serialized model.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
    
        from transformers import T5ForConditionalGeneration
    
        # test with `google-t5/t5-small`
>       model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:563: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_____________ MixedInt8T5Test.test_inference_without_keep_in_fp32 ______________

self = <bnb.test_mixed_int8.MixedInt8T5Test testMethod=test_inference_without_keep_in_fp32>

    def test_inference_without_keep_in_fp32(self):
        r"""
        Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly.
        `flan-t5-small` uses `T5DenseGatedActDense` whereas `google-t5/t5-small` uses `T5DenseReluDense`. We need to test
        both cases.
        """
        from transformers import T5ForConditionalGeneration
    
        modules = T5ForConditionalGeneration._keep_in_fp32_modules
        T5ForConditionalGeneration._keep_in_fp32_modules = None
    
        # test with `google-t5/t5-small`
>       model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:515: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
______________ MixedInt8ModelClassesTest.test_correct_head_class _______________

self = <bnb.test_mixed_int8.MixedInt8ModelClassesTest testMethod=test_correct_head_class>

    def setUp(self):
        super().setUp()
        # model_name
        self.model_name = "bigscience/bloom-560m"
        self.seq_to_seq_name = "google-t5/t5-small"
    
        # Different types of model
    
>       self.base_model = AutoModel.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:593: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_____________________ MixedInt8TestPipeline.test_pipeline ______________________

self = <bnb.test_mixed_int8.MixedInt8TestPipeline testMethod=test_pipeline>

    def test_pipeline(self):
        r"""
        The aim of this test is to verify that the mixed int8 is compatible with `pipeline` from transformers. Since
        we used pipline for inference speed benchmarking we want to make sure that this feature does not break anything
        on pipline.
        """
        # self._clear_cuda_cache()
>       self.pipe = pipeline(
            "text-generation",
            model=self.model_name,
            model_kwargs={"device_map": "auto", "load_in_8bit": True},
            max_new_tokens=self.MAX_NEW_TOKENS,
        )

tests/quantization/bnb/test_mixed_int8.py:656: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/pipelines/__init__.py:895: in pipeline
    framework, model = infer_framework_load_model(
src/transformers/pipelines/base.py:286: in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_________________ MixedInt8TestMultiGpu.test_multi_gpu_loading _________________

self = <bnb.test_mixed_int8.MixedInt8TestMultiGpu testMethod=test_multi_gpu_loading>

    def test_multi_gpu_loading(self):
        r"""
        This tests that the model has been loaded and can be used correctly on a multi-GPU setup.
        Let's just try to load a model on 2 GPUs and see if it works. The model we test has ~2GB of total, 3GB should suffice
        """
    
>       model_parallel = AutoModelForCausalLM.from_pretrained(
            self.model_name, load_in_8bit=True, device_map="balanced"
        )

tests/quantization/bnb/test_mixed_int8.py:680: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_______ MixedInt8TestCpuGpu.test_cpu_gpu_disk_loading_custom_device_map ________

self = <bnb.test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_disk_loading_custom_device_map>

    def test_cpu_gpu_disk_loading_custom_device_map(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.
        This time we also add `disk` on the device_map.
        """
        device_map = {
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": "cpu",
            "lm_head": 0,
            "transformer.h": 1,
            "transformer.ln_f": "disk",
        }
        bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_8bit=True)
        with tempfile.TemporaryDirectory() as tmpdirname:
            # Load model
>           model_8bit = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=device_map,
                quantization_config=bnb_config,
                offload_folder=tmpdirname,
            )

tests/quantization/bnb/test_mixed_int8.py:804: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
____ MixedInt8TestCpuGpu.test_cpu_gpu_disk_loading_custom_device_map_kwargs ____

self = <bnb.test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_disk_loading_custom_device_map_kwargs>

    def test_cpu_gpu_disk_loading_custom_device_map_kwargs(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.
        This time we also add `disk` on the device_map - using the kwargs directly instead of the quantization config
        """
        device_map = {
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": "cpu",
            "lm_head": 0,
            "transformer.h": 1,
            "transformer.ln_f": "disk",
        }
        with tempfile.TemporaryDirectory() as tmpdirname:
            # Load model
>           model_8bit = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=device_map,
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True,
                offload_folder=tmpdirname,
            )

tests/quantization/bnb/test_mixed_int8.py:830: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
__________ MixedInt8TestCpuGpu.test_cpu_gpu_loading_custom_device_map __________

self = <bnb.test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_loading_custom_device_map>

    def test_cpu_gpu_loading_custom_device_map(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.
        This time the device map is more organized than the test above and uses the abstraction
        `transformer.h` to encapsulate all the decoder layers.
        """
        device_map = {
            "transformer.word_embeddings": "cpu",
            "transformer.word_embeddings_layernorm": "cpu",
            "lm_head": "cpu",
            "transformer.h": 0,
            "transformer.ln_f": 1,
        }
        bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_8bit=True)
    
        # Load model
>       model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map=device_map,
            quantization_config=bnb_config,
        )

tests/quantization/bnb/test_mixed_int8.py:778: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
__________ MixedInt8TestCpuGpu.test_cpu_gpu_loading_random_device_map __________

self = <bnb.test_mixed_int8.MixedInt8TestCpuGpu testMethod=test_cpu_gpu_loading_random_device_map>

    def test_cpu_gpu_loading_random_device_map(self):
        r"""
        A test to check is dispatching a model on cpu & gpu works correctly using a random `device_map`.
        """
        device_map = {
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": 0,
            "lm_head": 0,
            "transformer.h.0": "cpu",
            "transformer.h.1": "cpu",
            "transformer.h.2": 0,
            "transformer.h.3": 0,
            "transformer.h.4": 0,
            "transformer.h.5": 0,
            "transformer.h.6": 0,
            "transformer.h.7": 0,
            "transformer.h.8": 0,
            "transformer.h.9": 1,
            "transformer.h.10": 0,
            "transformer.h.11": 1,
            "transformer.h.12": 0,
            "transformer.h.13": 0,
            "transformer.h.14": 1,
            "transformer.h.15": 0,
            "transformer.h.16": 0,
            "transformer.h.17": 1,
            "transformer.h.18": 1,
            "transformer.h.19": 0,
            "transformer.h.20": 1,
            "transformer.h.21": 1,
            "transformer.h.22": 0,
            "transformer.h.23": 0,
            "transformer.ln_f": 1,
        }
    
        bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_8bit=True)
    
>       model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map=device_map,
            quantization_config=bnb_config,
        )

tests/quantization/bnb/test_mixed_int8.py:751: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
_____________________ MixedInt8TestTraining.test_training ______________________

self = <bnb.test_mixed_int8.MixedInt8TestTraining testMethod=test_training>

    def test_training(self):
        if version.parse(importlib.metadata.version("bitsandbytes")) < version.parse("0.37.0"):
            self.skipTest(reason="This test requires bitsandbytes>=0.37.0")
    
        # Step 1: freeze all parameters
        model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)
    
        if torch.cuda.is_available():
            self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})
        else:
>           self.assertTrue(all(param.device.type == "cpu" for param in model.parameters()))
E           AssertionError: False is not true

tests/quantization/bnb/test_mixed_int8.py:860: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
______________ MixedInt8GPT2Test.test_device_and_dtype_assignment ______________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_device_and_dtype_assignment>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_________________ MixedInt8GPT2Test.test_fp32_int8_conversion __________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_fp32_int8_conversion>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
___________________ MixedInt8GPT2Test.test_generate_quality ____________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_generate_quality>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
________________ MixedInt8GPT2Test.test_generate_quality_config ________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_generate_quality_config>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
______________ MixedInt8GPT2Test.test_generate_quality_dequantize ______________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_generate_quality_dequantize>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
________________ MixedInt8GPT2Test.test_get_keys_to_not_convert ________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_get_keys_to_not_convert>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_______ MixedInt8GPT2Test.test_get_keys_to_not_convert_trust_remote_code _______

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_get_keys_to_not_convert_trust_remote_code>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_________________ MixedInt8GPT2Test.test_int8_from_pretrained __________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_from_pretrained>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
__________________ MixedInt8GPT2Test.test_int8_serialization ___________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_serialization>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_____________ MixedInt8GPT2Test.test_int8_serialization_regression _____________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_serialization_regression>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
______________ MixedInt8GPT2Test.test_int8_serialization_sharded _______________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_int8_serialization_sharded>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
____________________ MixedInt8GPT2Test.test_linear_are_8bit ____________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_linear_are_8bit>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
_______________________ MixedInt8GPT2Test.test_llm_skip ________________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_llm_skip>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
___________________ MixedInt8GPT2Test.test_memory_footprint ____________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_memory_footprint>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
____________________ MixedInt8GPT2Test.test_original_dtype _____________________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_original_dtype>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
________ MixedInt8GPT2Test.test_quantization_config_json_serialization _________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_quantization_config_json_serialization>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
___________ MixedInt8GPT2Test.test_raise_if_config_and_load_in_8bit ____________

self = <bnb.test_mixed_int8.MixedInt8GPT2Test testMethod=test_raise_if_config_and_load_in_8bit>

    def setUp(self):
        super().setUp()
    
        # Models and tokenizer
        self.model_fp16 = AutoModelForCausalLM.from_pretrained(
            self.model_name, torch_dtype=torch.float16, device_map="auto"
        )
>       self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map="auto")

tests/quantization/bnb/test_mixed_int8.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/auto/auto_factory.py:564: in from_pretrained
    return model_class.from_pretrained(
src/transformers/modeling_utils.py:3942: in from_pretrained
    ) = cls._load_pretrained_model(
src/transformers/modeling_utils.py:4416: in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
src/transformers/modeling_utils.py:939: in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
src/transformers/quantizers/quantizer_bnb_8bit.py:231: in create_quantized_param
    new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)
../bnb/bitsandbytes/nn/modules.py:641: in to
    return self.cuda(device)
../bnb/bitsandbytes/nn/modules.py:588: in cuda
    B = self.data.contiguous().half().cuda(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/cuda/__init__.py:289: AssertionError
----------------------------- Captured stderr call -----------------------------
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
=============================== warnings summary ===============================
../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/utils/cpp_extension.py:28
  /home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/utils/cpp_extension.py:28: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    from pkg_resources import packaging  # type: ignore[attr-defined]

../../.condax/mamba/envs/bnb/lib/python3.10/site-packages/_pytest/config/__init__.py:1448
  /home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/_pytest/config/__init__.py:1448: PytestConfigWarning: Unknown config option: doctest_glob
  
    self._warn_or_fail_if_strict(f"Unknown config option: {key}\n")

tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_device_and_dtype_assignment
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_generate_quality
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_generate_quality_config
tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_generate_quality_dequantize
  /home/sdp/src/transformers/src/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on xpu, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.
    warnings.warn(

tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert_trust_remote_code
  /home/sdp/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7/configuration_mpt.py:90: DeprecationWarning: verbose argument for MPTConfig is now ignored and will be removed. Use python_log_level instead.
    warnings.warn(DeprecationWarning('verbose argument for MPTConfig is now ignored and will be removed. Use python_log_level instead.'))

tests/quantization/bnb/test_mixed_int8.py::MixedInt8Test::test_get_keys_to_not_convert_trust_remote_code
  /home/sdp/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/ada218f9a93b5f1c6dce48a4cc9ff01fcba431e7/configuration_mpt.py:97: UserWarning: alibi is turned on, setting `learned_pos_emb` to `False.`
    warnings.warn(f'alibi is turned on, setting `learned_pos_emb` to `False.`')

tests/quantization/bnb/test_mixed_int8.py: 18 warnings
  /home/sdp/src/transformers/src/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
    warnings.warn(

tests/quantization/bnb/test_mixed_int8.py::MixedInt8TestTraining::test_training
  /home/sdp/.condax/mamba/envs/bnb/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
    return self.fget.__get__(instance, owner)()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
======== 42 failed, 3 passed, 27 warnings, 1 error in 108.72s (0:01:48) ========
